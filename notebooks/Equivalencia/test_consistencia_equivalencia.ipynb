{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bert import BERT\n",
    "from utils_vocab import BasicTokenizer, BERTDatasetNoLabels, evaluate\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset = 'equivalencia_5.csv'\n",
    "# raw_dataset = 'equivalencia_10.csv'\n",
    "# raw_dataset = 'equivalencia_15.csv'\n",
    "raw_dataset = 'equivalencia_20.csv'\n",
    "\n",
    "# tokenizer_file = 'tokenizer_5.pkl'\n",
    "# tokenizer_file = 'tokenizer_10.pkl'\n",
    "# tokenizer_file = 'tokenizer_15.pkl'\n",
    "tokenizer_file = 'tokenizer_20.pkl'\n",
    "\n",
    "# path_model = 'equivalencia_5.pt'\n",
    "# path_model = 'equivalencia_10.pt'\n",
    "# path_model = 'equivalencia_15.pt'\n",
    "path_model = 'equivalencia_20.pt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size: 86\n"
     ]
    }
   ],
   "source": [
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "PAD_IDX = 1\n",
    "CLS_IDX = 2\n",
    "SEP_IDX = 3\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer.create_using_stoi(simple_tokenizer, special_symbols, tokenizer_file)\n",
    "print('vocabulary_size:', tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar datos y crear dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2612399, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(raw_dataset)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "\n",
    "    for sentence1, sentence2, is_next in zip(sentences1, sentences2, is_nexts):\n",
    "        # Tokenize each sentence\n",
    "        tokens1 = tokenizer.encode(sentence1).ids\n",
    "        tokens2 = tokenizer.encode(sentence2).ids\n",
    "        bert_input = ([CLS_IDX] + tokens1 + [SEP_IDX], tokens2 + [SEP_IDX])\n",
    "\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        #convert to tensors\n",
    "        if to_tensor:\n",
    "\n",
    "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "            # bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
    "            # bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            # segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "        else:\n",
    "          # Flatten the padded inputs and labels\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, segment_labels_final, is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df.iloc[:, 0]\n",
    "sentences2 = df.iloc[:, 1]\n",
    "is_nexts = df.iloc[:, 2]\n",
    "bert_inputs_final, segment_labels_final, is_nexts_final = direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_final.shape=(2612399, 3)\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'BERT Input': bert_inputs_final,\n",
    "    'Segment Label': segment_labels_final,\n",
    "    'Is Next': is_nexts_final\n",
    "})\n",
    "print(f'{df_final.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BERTDatasetNoLabels(df_final)\n",
    "bert_inputs, segment_labels, is_nexts = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "print(device)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, segment_labels_batch, is_nexts_batch = [], [], []\n",
    "\n",
    "    for bert_input, segment_label, is_next in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final.to(device), segment_labels_final.to(device), is_nexts_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert_embedding): BERTEmbedding(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(86, 16)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (segment_embedding): Embedding(3, 16)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (nextsentenceprediction): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (masked_language): Linear(in_features=16, out_features=86, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = tokenizer.get_vocab_size()  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 4  # Number of Transformer layers\n",
    "initial_heads = 4\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_model, weights_only=True,map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 69, 50, 15, 38, 68, 42, 3, 85, 50, 15, 38, 68, 42, 3, 1, 1, 1, 1],\n",
       " [2, 5, 6, 62, 14, 36, 68, 47, 3, 69, 62, 14, 5, 36, 67, 5, 47, 3, 1, 1],\n",
       " [2, 6, 56, 27, 37, 67, 31, 3, 69, 56, 27, 37, 67, 31, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 63, 28, 9, 67, 46, 3, 69, 63, 28, 5, 9, 68, 5, 46, 3, 1, 1],\n",
       " [2, 69, 63, 8, 29, 68, 45, 3, 85, 63, 8, 29, 68, 45, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 66, 10, 46, 3, 85, 66, 10, 46, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 69, 60, 12, 36, 68, 37, 3, 85, 60, 12, 36, 68, 37, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 56, 42, 68, 32, 3, 85, 56, 42, 68, 32, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 6, 55, 10, 43, 67, 47, 3, 69, 55, 10, 43, 67, 47, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 61, 23, 38, 67, 42, 3, 69, 61, 23, 5, 38, 67, 5, 42, 3, 1, 1],\n",
       " [2, 6, 54, 21, 46, 3, 69, 54, 21, 46, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 53, 26, 44, 67, 37, 3, 69, 53, 26, 5, 44, 67, 5, 37, 3, 1, 1],\n",
       " [2, 69, 61, 27, 9, 67, 38, 3, 85, 61, 27, 9, 67, 38, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 56, 17, 5, 41, 68, 5, 29, 3, 85, 56, 17, 41, 67, 29, 3, 1, 1, 1],\n",
       " [2, 5, 69, 58, 12, 37, 67, 46, 3, 6, 58, 12, 5, 37, 68, 5, 46, 3, 1, 1],\n",
       " [2, 5, 69, 60, 16, 36, 67, 29, 3, 6, 60, 16, 5, 36, 68, 5, 29, 3, 1, 1],\n",
       " [2, 5, 6, 51, 24, 37, 68, 36, 3, 69, 51, 24, 5, 37, 68, 5, 36, 3, 1, 1],\n",
       " [2, 5, 69, 61, 14, 46, 68, 42, 3, 85, 61, 14, 46, 68, 42, 3, 1, 1, 1, 1],\n",
       " [2, 5, 69, 58, 23, 46, 68, 35, 3, 85, 58, 23, 46, 68, 35, 3, 1, 1, 1, 1],\n",
       " [2, 69, 63, 13, 47, 68, 42, 3, 85, 63, 13, 47, 68, 42, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 57, 27, 31, 67, 46, 3, 1, 1, 5, 69, 57, 27, 5, 31, 68, 5, 46, 3],\n",
       " [2, 6, 64, 12, 31, 67, 36, 3, 1, 1, 5, 69, 64, 12, 5, 31, 68, 5, 36, 3],\n",
       " [2, 6, 53, 24, 36, 67, 32, 3, 69, 53, 24, 36, 67, 32, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 48, 15, 5, 32, 3, 85, 48, 15, 32, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 62, 8, 33, 68, 30, 3, 85, 62, 8, 33, 68, 30, 3, 1, 1, 1, 1],\n",
       " [2, 6, 63, 28, 5, 9, 67, 5, 44, 3, 85, 63, 28, 9, 68, 44, 3, 1, 1, 1],\n",
       " [2, 69, 58, 11, 41, 67, 46, 3, 85, 58, 11, 41, 67, 46, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 58, 19, 40, 68, 43, 3, 1, 1, 5, 69, 58, 19, 5, 40, 67, 5, 43, 3],\n",
       " [2, 6, 49, 16, 35, 68, 38, 3, 69, 49, 16, 35, 68, 38, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 51, 24, 35, 68, 44, 3, 85, 51, 24, 35, 68, 44, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 55, 14, 32, 67, 34, 3, 85, 55, 14, 32, 67, 34, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 49, 13, 37, 67, 45, 3, 69, 49, 13, 5, 37, 67, 5, 45, 3, 1, 1],\n",
       " [2, 6, 53, 23, 34, 68, 40, 3, 1, 1, 5, 69, 53, 23, 5, 34, 67, 5, 40, 3],\n",
       " [2, 6, 48, 25, 31, 67, 46, 3, 1, 1, 5, 69, 48, 25, 5, 31, 68, 5, 46, 3],\n",
       " [2, 6, 58, 28, 5, 39, 67, 5, 36, 3, 85, 58, 28, 39, 68, 36, 3, 1, 1, 1],\n",
       " [2, 69, 59, 28, 9, 3, 85, 59, 28, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 63, 14, 41, 68, 32, 3, 6, 63, 14, 5, 41, 67, 5, 32, 3, 1, 1],\n",
       " [2, 5, 69, 57, 14, 38, 67, 43, 3, 85, 57, 14, 38, 67, 43, 3, 1, 1, 1, 1],\n",
       " [2, 69, 65, 22, 35, 67, 29, 3, 85, 65, 22, 35, 67, 29, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 56, 31, 3, 85, 56, 31, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 6, 60, 18, 5, 43, 67, 5, 30, 3, 85, 60, 18, 43, 68, 30, 3, 1, 1, 1],\n",
       " [2, 5, 6, 54, 24, 46, 67, 41, 3, 69, 54, 24, 5, 46, 67, 5, 41, 3, 1, 1],\n",
       " [2, 5, 6, 52, 19, 41, 68, 36, 3, 69, 52, 19, 5, 41, 67, 5, 36, 3, 1, 1],\n",
       " [2, 5, 69, 65, 25, 34, 67, 43, 3, 85, 65, 25, 34, 67, 43, 3, 1, 1, 1, 1],\n",
       " [2, 5, 6, 62, 19, 40, 68, 41, 3, 69, 62, 19, 5, 40, 68, 5, 41, 3, 1, 1],\n",
       " [2, 5, 69, 63, 13, 47, 67, 30, 3, 6, 63, 13, 5, 47, 68, 5, 30, 3, 1, 1],\n",
       " [2, 5, 69, 64, 21, 47, 67, 34, 3, 85, 64, 21, 47, 67, 34, 3, 1, 1, 1, 1],\n",
       " [2, 6, 66, 13, 5, 37, 67, 5, 32, 3, 85, 66, 13, 37, 68, 32, 3, 1, 1, 1],\n",
       " [2, 5, 69, 58, 37, 67, 35, 3, 6, 58, 5, 37, 68, 5, 35, 3, 1, 1, 1, 1],\n",
       " [2, 69, 54, 21, 41, 68, 47, 3, 85, 54, 21, 41, 68, 47, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 49, 10, 35, 68, 43, 3, 1, 1, 5, 69, 49, 10, 5, 35, 67, 5, 43, 3],\n",
       " [2, 5, 6, 48, 18, 30, 67, 40, 3, 69, 48, 18, 5, 30, 67, 5, 40, 3, 1, 1],\n",
       " [2, 69, 56, 12, 35, 68, 9, 3, 85, 56, 12, 35, 68, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 51, 28, 5, 39, 67, 5, 32, 3, 85, 51, 28, 39, 68, 32, 3, 1, 1, 1],\n",
       " [2, 5, 69, 57, 15, 31, 68, 36, 3, 85, 57, 15, 31, 68, 36, 3, 1, 1, 1, 1],\n",
       " [2, 5, 69, 49, 12, 42, 67, 46, 3, 6, 49, 12, 5, 42, 68, 5, 46, 3, 1, 1],\n",
       " [2, 5, 69, 60, 22, 44, 67, 31, 3, 6, 60, 22, 5, 44, 68, 5, 31, 3, 1, 1],\n",
       " [2, 69, 58, 19, 31, 67, 40, 3, 85, 58, 19, 31, 67, 40, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 62, 25, 5, 47, 3, 85, 62, 25, 47, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 6, 54, 5, 32, 67, 5, 42, 3, 85, 54, 32, 68, 42, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 48, 26, 39, 67, 29, 3, 69, 48, 26, 5, 39, 68, 5, 29, 3, 1, 1],\n",
       " [2, 6, 54, 26, 37, 68, 35, 3, 1, 1, 5, 69, 54, 26, 5, 37, 67, 5, 35, 3],\n",
       " [2, 6, 55, 26, 38, 68, 43, 3, 1, 1, 5, 69, 55, 26, 5, 38, 67, 5, 43, 3],\n",
       " [2, 5, 69, 7, 23, 39, 68, 45, 3, 85, 7, 23, 39, 68, 45, 3, 1, 1, 1, 1],\n",
       " [2, 6, 64, 14, 36, 67, 9, 3, 69, 64, 14, 36, 67, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 58, 25, 9, 68, 29, 3, 85, 58, 25, 9, 68, 29, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 62, 24, 5, 29, 68, 5, 42, 3, 85, 62, 24, 29, 67, 42, 3, 1, 1, 1],\n",
       " [2, 5, 6, 49, 10, 35, 67, 29, 3, 69, 49, 10, 5, 35, 67, 5, 29, 3, 1, 1],\n",
       " [2, 69, 58, 11, 32, 68, 43, 3, 85, 58, 11, 32, 68, 43, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 55, 17, 39, 67, 34, 3, 69, 55, 17, 39, 67, 34, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 61, 24, 30, 68, 29, 3, 1, 1, 5, 69, 61, 24, 5, 30, 67, 5, 29, 3],\n",
       " [2, 6, 62, 16, 33, 67, 36, 3, 69, 62, 16, 33, 67, 36, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 65, 14, 31, 68, 29, 3, 6, 65, 14, 5, 31, 67, 5, 29, 3, 1, 1],\n",
       " [2, 6, 62, 14, 38, 68, 35, 3, 69, 62, 14, 38, 68, 35, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 64, 20, 42, 68, 31, 3, 69, 64, 20, 5, 42, 67, 5, 31, 3, 1, 1],\n",
       " [2, 5, 6, 52, 24, 30, 67, 41, 3, 69, 52, 24, 5, 30, 67, 5, 41, 3, 1, 1],\n",
       " [2, 5, 69, 53, 21, 33, 67, 38, 3, 85, 53, 21, 33, 67, 38, 3, 1, 1, 1, 1],\n",
       " [2, 6, 61, 16, 33, 67, 32, 3, 1, 1, 5, 69, 61, 16, 5, 33, 68, 5, 32, 3],\n",
       " [2, 6, 60, 10, 30, 68, 36, 3, 1, 1, 5, 69, 60, 10, 5, 30, 67, 5, 36, 3],\n",
       " [2, 6, 61, 15, 5, 46, 67, 5, 30, 3, 85, 61, 15, 46, 68, 30, 3, 1, 1, 1],\n",
       " [2, 5, 6, 53, 17, 30, 68, 34, 3, 69, 53, 17, 5, 30, 68, 5, 34, 3, 1, 1],\n",
       " [2, 5, 69, 57, 18, 29, 67, 38, 3, 6, 57, 18, 5, 29, 68, 5, 38, 3, 1, 1],\n",
       " [2, 5, 69, 59, 28, 39, 67, 32, 3, 85, 59, 28, 39, 67, 32, 3, 1, 1, 1, 1],\n",
       " [2, 6, 64, 24, 5, 9, 68, 5, 34, 3, 85, 64, 24, 9, 67, 34, 3, 1, 1, 1],\n",
       " [2, 69, 51, 15, 36, 67, 9, 3, 85, 51, 15, 36, 67, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 49, 18, 45, 67, 39, 3, 85, 49, 18, 45, 67, 39, 3, 1, 1, 1, 1],\n",
       " [2, 6, 50, 17, 38, 68, 39, 3, 69, 50, 17, 38, 68, 39, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 53, 11, 40, 67, 29, 3, 69, 53, 11, 40, 67, 29, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 62, 26, 37, 67, 33, 3, 69, 62, 26, 5, 37, 67, 5, 33, 3, 1, 1],\n",
       " [2, 6, 60, 11, 35, 68, 9, 3, 69, 60, 11, 35, 68, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 69, 53, 20, 9, 68, 39, 3, 85, 53, 20, 9, 68, 39, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 54, 28, 5, 35, 68, 5, 30, 3, 85, 54, 28, 35, 67, 30, 3, 1, 1, 1],\n",
       " [2, 5, 69, 65, 12, 44, 67, 38, 3, 85, 65, 12, 44, 67, 38, 3, 1, 1, 1, 1],\n",
       " [2, 6, 52, 15, 47, 68, 42, 3, 69, 52, 15, 47, 68, 42, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 61, 13, 5, 33, 68, 5, 40, 3, 85, 61, 13, 33, 67, 40, 3, 1, 1, 1],\n",
       " [2, 6, 55, 13, 5, 32, 67, 5, 46, 3, 85, 55, 13, 32, 68, 46, 3, 1, 1, 1],\n",
       " [2, 6, 63, 25, 40, 67, 44, 3, 1, 1, 5, 69, 63, 25, 5, 40, 68, 5, 44, 3],\n",
       " [2, 6, 62, 21, 32, 68, 31, 3, 1, 1, 5, 69, 62, 21, 5, 32, 67, 5, 31, 3],\n",
       " [2, 5, 69, 66, 13, 35, 67, 45, 3, 6, 66, 13, 5, 35, 68, 5, 45, 3, 1, 1],\n",
       " [2, 5, 69, 52, 24, 36, 67, 30, 3, 85, 52, 24, 36, 67, 30, 3, 1, 1, 1, 1],\n",
       " [2, 5, 69, 55, 22, 29, 68, 30, 3, 6, 55, 22, 5, 29, 67, 5, 30, 3, 1, 1],\n",
       " [2, 69, 48, 25, 45, 68, 40, 3, 85, 48, 25, 45, 68, 40, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 54, 27, 31, 68, 29, 3, 6, 54, 27, 5, 31, 67, 5, 29, 3, 1, 1],\n",
       " [2, 5, 69, 55, 23, 44, 67, 29, 3, 85, 55, 23, 44, 67, 29, 3, 1, 1, 1, 1],\n",
       " [2, 5, 6, 64, 20, 9, 67, 35, 3, 69, 64, 20, 5, 9, 68, 5, 35, 3, 1, 1],\n",
       " [2, 6, 63, 34, 67, 31, 3, 69, 63, 34, 67, 31, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 48, 26, 35, 67, 34, 3, 6, 48, 26, 5, 35, 68, 5, 34, 3, 1, 1],\n",
       " [2, 5, 6, 54, 25, 31, 68, 34, 3, 69, 54, 25, 5, 31, 67, 5, 34, 3, 1, 1],\n",
       " [2, 5, 69, 59, 18, 39, 67, 9, 3, 85, 59, 18, 39, 67, 9, 3, 1, 1, 1, 1],\n",
       " [2, 6, 55, 19, 29, 67, 39, 3, 1, 1, 5, 69, 55, 19, 5, 29, 68, 5, 39, 3],\n",
       " [2, 6, 59, 24, 5, 34, 68, 5, 39, 3, 85, 59, 24, 34, 67, 39, 3, 1, 1, 1],\n",
       " [2, 5, 6, 55, 16, 36, 67, 41, 3, 69, 55, 16, 5, 36, 68, 5, 41, 3, 1, 1],\n",
       " [2, 5, 69, 50, 23, 35, 67, 29, 3, 85, 50, 23, 35, 67, 29, 3, 1, 1, 1, 1],\n",
       " [2, 6, 56, 15, 5, 33, 67, 5, 36, 3, 85, 56, 15, 33, 68, 36, 3, 1, 1, 1],\n",
       " [2, 69, 57, 16, 33, 68, 9, 3, 85, 57, 16, 33, 68, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 56, 11, 33, 68, 9, 3, 85, 56, 11, 33, 68, 9, 3, 1, 1, 1, 1],\n",
       " [2, 6, 50, 15, 5, 30, 67, 5, 29, 3, 85, 50, 15, 30, 68, 29, 3, 1, 1, 1],\n",
       " [2, 5, 69, 49, 18, 45, 67, 30, 3, 85, 49, 18, 45, 67, 30, 3, 1, 1, 1, 1],\n",
       " [2, 5, 6, 53, 20, 33, 67, 47, 3, 69, 53, 20, 5, 33, 67, 5, 47, 3, 1, 1],\n",
       " [2, 6, 52, 14, 42, 68, 31, 3, 69, 52, 14, 42, 68, 31, 3, 1, 1, 1, 1, 1],\n",
       " [2, 6, 52, 20, 32, 67, 47, 3, 1, 1, 5, 69, 52, 20, 5, 32, 68, 5, 47, 3],\n",
       " [2, 5, 6, 52, 13, 33, 68, 45, 3, 69, 52, 13, 5, 33, 68, 5, 45, 3, 1, 1],\n",
       " [2, 6, 66, 23, 5, 34, 67, 5, 31, 3, 85, 66, 23, 34, 68, 31, 3, 1, 1, 1],\n",
       " [2, 5, 6, 58, 13, 39, 68, 34, 3, 69, 58, 13, 5, 39, 68, 5, 34, 3, 1, 1],\n",
       " [2, 6, 66, 27, 5, 32, 67, 5, 43, 3, 85, 66, 27, 32, 68, 43, 3, 1, 1, 1],\n",
       " [2, 5, 6, 66, 18, 34, 67, 36, 3, 69, 66, 18, 5, 34, 67, 5, 36, 3, 1, 1],\n",
       " [2, 6, 53, 19, 33, 67, 32, 3, 69, 53, 19, 33, 67, 32, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 69, 50, 27, 39, 68, 47, 3, 6, 50, 27, 5, 39, 67, 5, 47, 3, 1, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.cpu().numpy().transpose(1, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    next_sentence_prediction, masked_language = model(bert_inputs.to(device), segment_labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_sentences(bert_input):\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0] + pair[1]\n",
    "\n",
    "    tokens = bert_input.cpu().numpy().tolist()\n",
    "    assert(len(tokens) % 2 == 0), len(tokens)\n",
    "    try:\n",
    "        len_sents = tokens.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(tokens)\n",
    "        print(tokenizer.decode(tokens))\n",
    "        raise Exception(e)\n",
    "    first = tokens[1:len_sents - 1]\n",
    "    second_ = tokens[len_sents:]\n",
    "    try:\n",
    "        sep_in_second = second_.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(second_)\n",
    "        print(tokenizer.decode(second_))\n",
    "        raise Exception(e)\n",
    "    second = second_[:sep_in_second]\n",
    "    bert_input = ([CLS_IDX] + second + [SEP_IDX], first + [SEP_IDX])\n",
    "    # Create segment labels for each pair of sentences\n",
    "    segment_label = ([1] * len(bert_input[0]), [2] * len(bert_input[1]))\n",
    "    # Zero-pad the bert_input and bert_label and segment_label\n",
    "    bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "    bert_input_padded = torch.tensor(bert_input_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "    segment_label_padded = torch.tensor(segment_label_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    return bert_input_padded, segment_label_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(input, segment_label, is_next, device):\n",
    "    with torch.no_grad():\n",
    "        model_prediction, _ = model(input.to(device), segment_label.to(device))\n",
    "    logits = torch.softmax(model_prediction, dim=1)\n",
    "    prediction = torch.argmax(logits, dim=1)     \n",
    "    correct = prediction == is_next\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b64ead6a2142d1a0c12f85eb78f722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7141874856749942\n",
      "Accuracy: 0.5554367201426025\n"
     ]
    }
   ],
   "source": [
    "test = list()\n",
    "golden = list()\n",
    "for bert_inputs, segment_labels, is_nexts in tqdm(dataloader):\n",
    "    corrects = get_prediction(\n",
    "        input=bert_inputs,\n",
    "        segment_label=segment_labels,\n",
    "        is_next=is_nexts,\n",
    "        device=device\n",
    "    )\n",
    "    for idx, correct in enumerate(corrects):\n",
    "        if correct:\n",
    "            bert_input = bert_inputs.transpose(1,0)[idx]\n",
    "            bert_input_reversed, segment_label = invert_sentences(bert_input)\n",
    "            check = get_prediction(\n",
    "                input=bert_input_reversed, \n",
    "                segment_label=segment_label,\n",
    "                is_next=is_nexts[idx],\n",
    "                device=device\n",
    "            )\n",
    "            test.append(check.cpu().item())\n",
    "            golden.append(is_nexts[idx].cpu())\n",
    "acc = accuracy_score(golden, test)\n",
    "f1 = f1_score(golden, test)    \n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
