{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cde1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.utils.barrido import NNTrainer\n",
    "from src.config import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb81d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = PATHS[\"training_data_folder\"]\n",
    "tokenizer_folder = PATHS[\"tokenizer_folder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a4db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NNTrainer(\n",
    "    nombre='equivalencia',\n",
    "    tokenizer_file=tokenizer_folder / 'tokenizer_5.pkl',\n",
    "    path_dataset=dataset_folder / 'bert_data_equivalencia_5.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58929b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9d000669e6438688596b6ab5acdbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ejecutando punto...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1d4efdac67402892528f4e4c08e0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94adc2d642f541b3a7c1bca051953075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bcfdda8a6c453881b442cfb98028eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1cc0d05bc44b1890875268ae0ead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24229289c2344955971d759d5c53158b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236f89118f664345a1e7ab12aef09653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': inf, 'accurracy': inf, 'f1': inf, 'hiperparametros': (16, 10, 6, 0.3, 'adam', 1e-05, 0.3, 16), 'message': \"Error: 'accuracy'\"}, {'loss': inf, 'accurracy': inf, 'f1': inf, 'hiperparametros': (16, 4, 4, 0.4, 'sgd', 0.0001, 0.1, 32), 'message': \"Error: 'accuracy'\"}]\n",
      "Â¡Barrido terminado!\n"
     ]
    }
   ],
   "source": [
    "trainer.ejecutar_barrido()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b06c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1: inf\n",
      "Best embedding_dim: 16\n",
      "Best net_layers: 10\n",
      "Best net_heads: 6\n",
      "Best dropout: 0.3\n",
      "Best optimizer_class: adam\n",
      "Best learning_rate_initial: 1e-05\n",
      "Best gamma: 0.3\n",
      "Best batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "mejor_punto_idx = trainer.retornar_mejor_combinacion_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3db856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': inf,\n",
       "  'accurracy': inf,\n",
       "  'f1': inf,\n",
       "  'hiperparametros': (16, 10, 6, 0.3, 'adam', 1e-05, 0.3, 16),\n",
       "  'message': \"Error: 'accuracy'\"},\n",
       " {'loss': inf,\n",
       "  'accurracy': inf,\n",
       "  'f1': inf,\n",
       "  'hiperparametros': (16, 4, 4, 0.4, 'sgd', 0.0001, 0.1, 32),\n",
       "  'message': \"Error: 'accuracy'\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44445986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2776025f27254e908a3f3b59001ced5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37133a02e66444a7a3a193fec35205c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abad322e9e9b480f8c454ef9a2e7c89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
      "c:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mN_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m mejor_punto \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpuntos[mejor_punto_idx]\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mejecutar_punto(mejor_punto)\n",
      "File \u001b[1;32mc:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:76\u001b[0m, in \u001b[0;36mNNTrainer.ejecutar_punto\u001b[1;34m(self, punto, save_log, progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model(batch_size, progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar)\n\u001b[0;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     78\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\andra\\Documents\\TestBERT\\notebooks\\Equivalencia\\../..\\src\\utils\\barrido.py:134\u001b[0m, in \u001b[0;36mNNTrainer.train_model\u001b[1;34m(self, batch_size, progress_bar)\u001b[0m\n\u001b[0;32m    131\u001b[0m mask_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn_mlm(masked_language\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, masked_language\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), bert_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m next_loss \u001b[38;5;241m+\u001b[39m mask_loss\n\u001b[1;32m--> 134\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    135\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andra\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.N_EPOCHS = 10\n",
    "mejor_punto = trainer.puntos[mejor_punto_idx]\n",
    "trainer.TRAINING_IN_EARNEST = True\n",
    "results = trainer.ejecutar_punto(mejor_punto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
