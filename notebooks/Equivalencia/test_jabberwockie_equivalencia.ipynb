{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Test Jabberwockie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from bert import BERT\n",
    "from utils_vocab import BasicTokenizer, BERTDataset, evaluate\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_file = 'tokenizer_5.pkl'\n",
    "# tokenizer_file = 'tokenizer_10.pkl'\n",
    "# tokenizer_file = 'tokenizer_15.pkl'\n",
    "tokenizer_file = 'tokenizer_20.pkl'\n",
    "\n",
    "# path_model = 'equivalencia_5.pt'\n",
    "# path_model = 'equivalencia_10.pt'\n",
    "# path_model = 'equivalencia_15.pt'\n",
    "path_model = 'equivalencia_20.pt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size: 86\n"
     ]
    }
   ],
   "source": [
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer.create_using_stoi(simple_tokenizer, special_symbols, tokenizer_file)\n",
    "print('vocabulary_size:', tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos Jabberwockie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10650, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERT Input</th>\n",
       "      <th>BERT Label</th>\n",
       "      <th>Segment Label</th>\n",
       "      <th>Is Next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 5, 6, 25, 26, 27, 3, 24, 25, 26, 5, 4, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 5, 6, 25, 4, 27, 3, 24, 25, 28, 5, 27, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 28, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 5, 4, 25, 29, 27, 3, 24, 25, 29, 5, 27, 3, 1]</td>\n",
       "      <td>[1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          BERT Input  \\\n",
       "0   [2, 5, 6, 25, 26, 27, 3, 24, 25, 26, 5, 4, 3, 1]   \n",
       "1   [2, 5, 6, 25, 4, 27, 3, 24, 25, 28, 5, 27, 3, 1]   \n",
       "2  [2, 5, 4, 25, 29, 27, 3, 24, 25, 29, 5, 27, 3, 1]   \n",
       "\n",
       "                                    BERT Label  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 1, 1]   \n",
       "1  [1, 1, 1, 1, 28, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2   [1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                Segment Label  Is Next  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]        1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]        1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0]        1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dataset = 'bert_data_equivalencia_jabberwockie_5.csv'\n",
    "\n",
    "df = pd.read_csv(path_dataset)\n",
    "df.columns = ['BERT Input', 'BERT Label', 'Segment Label', 'Is Next']\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final.to(device), bert_labels_final.to(device), segment_labels_final.to(device), is_nexts_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_input=tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "        [ 6,  4,  6,  ..., 24,  5,  6],\n",
      "        [ 4, 24,  4,  ..., 38,  4, 39],\n",
      "        ...,\n",
      "        [ 1,  3,  5,  ...,  1,  1,  5],\n",
      "        [ 1,  1,  4,  ...,  1,  1, 33],\n",
      "        [ 1,  1,  3,  ...,  1,  1,  3]], device='cuda:0')\n",
      "bert_label=tensor([[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 1,  5,  1,  ...,  1,  1,  1],\n",
      "        [32,  1, 37,  ...,  1,  6,  1],\n",
      "        ...,\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 1,  1, 35,  ...,  1,  1,  1],\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')\n",
      "segment_label=tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 2, 2,  ..., 1, 1, 2],\n",
      "        [1, 1, 2,  ..., 1, 1, 2],\n",
      "        [1, 1, 2,  ..., 1, 1, 2]], device='cuda:0')\n",
      "is_next=tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "jabberwockie_dataset = BERTDataset(df)\n",
    "\n",
    "jabberwockie_dataloader = DataLoader(jabberwockie_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for bert_input, bert_label, segment_label, is_next in jabberwockie_dataloader:\n",
    "    print(f'{bert_input=}')\n",
    "    print(f'{bert_label=}')\n",
    "    print(f'{segment_label=}')\n",
    "    print(f'{is_next=}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert_embedding): BERTEmbedding(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(86, 16)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (segment_embedding): Embedding(3, 16)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (nextsentenceprediction): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (masked_language): Linear(in_features=16, out_features=86, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = tokenizer.get_vocab_size()  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 4  # Number of Transformer layers\n",
    "initial_heads = 4\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_model, weights_only=True,map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.0041118285235235\n",
      "Test f1 score: 0.8455674145066434\n",
      "Test accurracy: 0.800281690140845\n"
     ]
    }
   ],
   "source": [
    "loss, acc, f1 = evaluate(\n",
    "    dataloader=jabberwockie_dataloader,\n",
    "    model=model,\n",
    "    loss_fn_mlm=loss_fn_mlm,\n",
    "    loss_fn_nsp=loss_fn_nsp,\n",
    "    device=device\n",
    ")\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test f1 score: {f1}')\n",
    "print(f'Test accurracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
