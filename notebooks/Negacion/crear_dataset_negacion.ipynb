{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del dataset para oraciones de relaciones lógicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils_vocab import BasicTokenizer\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = 'negacion_5.csv'\n",
    "# raw_dataset = 'negacion_10.csv'\n",
    "# raw_dataset = 'negacion_15.csv'\n",
    "# raw_dataset = 'negacion_20.csv'\n",
    "\n",
    "tokenizer_file = 'tokenizer_5.pkl'\n",
    "# tokenizer_file = 'tokenizer_10.pkl'\n",
    "# tokenizer_file = 'tokenizer_15.pkl'\n",
    "# tokenizer_file = 'tokenizer_20.pkl'\n",
    "\n",
    "csv_file_path ='bert_data_negacion_5.csv'\n",
    "# csv_file_path ='bert_data_negacion_10.csv'\n",
    "# csv_file_path ='bert_data_negacion_15.csv'\n",
    "# csv_file_path ='bert_data_negacion_20.csv'\n",
    "\n",
    "jabberwockie_dataset = 'negacion_jabberwockie_5.csv'\n",
    "# csv_file_path ='bert_data_negacion_jabberwockie_5.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence 1</th>\n",
       "      <th>Sentence 2</th>\n",
       "      <th>Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>algún jufzyl drifla o brilca</td>\n",
       "      <td>todo jufzyl drifla o brilca</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21298</th>\n",
       "      <td>algún jufzyl drifla o brunza</td>\n",
       "      <td>todo jufzyl drifla o brunza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21299</th>\n",
       "      <td>algún jufzyl drifla o dernea</td>\n",
       "      <td>todo jufzyl drifla o dernea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Sentence 1                    Sentence 2  Relation\n",
       "21297  algún jufzyl drifla o brilca    todo jufzyl drifla o brilca         0\n",
       "21298  algún jufzyl drifla o brunza    todo jufzyl drifla o brunza         0\n",
       "21299  algún jufzyl drifla o dernea    todo jufzyl drifla o dernea         0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df1 = pd.read_csv(raw_dataset, names=['Sentence 1', 'Sentence 2', 'Relation'])\n",
    "words_jabberwockie = pd.read_csv(jabberwockie_dataset, names=['Sentence 1', 'Sentence 2', 'Relation'])\n",
    "words_df = pd.concat([words_df1, words_jabberwockie], ignore_index=True)\n",
    "words_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo abuelo acuerda ', 'todo abuelo aguanta ', 'todo abuelo ama ', 'todo abuelo amanece ', 'todo abuelo anochece ', 'todo actor acuerda ', 'todo actor aguanta ', 'todo actor ama ', 'todo actor amanece ', 'todo actor anochece ']\n",
      "42600\n"
     ]
    }
   ],
   "source": [
    "words = list(words_df.iloc[:,0].values)\n",
    "words += list(words_df.iloc[:,1].values)\n",
    "print(words[:10])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algún', 'dormínico', 'abuelo', 'o', 'ningún', 'ama', 'dernea', 'blicket', 'flakle', 'amarillo', 'amplio', 'bliscea', 'anochece', 'todo', 'brispado', 'alegre', 'alto', 'y', 'aguanta', 'amanece', 'acuerda', 'brilca', 'amargo', 'flexivo', 'albañil', 'flajuf', 'jufmoq', 'drifla', 'claribundo', 'no', 'actor', 'agujero', 'jufzyl', 'alce', 'florido', 'brunza']\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "y = [w.strip().split(' ')  for w in words]\n",
    "y = [x for w in y for x in w]\n",
    "y = [w for w in y if w != '']\n",
    "y = list(set(y))\n",
    "print(y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols for the tokenizer\n",
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "# Create tokenizer from words\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer(simple_tokenizer, special_symbols)\n",
    "tokenizer.initialize_from_iterable(words)\n",
    "\n",
    "# Save to file\n",
    "tokenizer.save(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.itos)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]', 'todo', 'abuelo', 'acuerda', 'aguanta', 'ama', 'amanece', 'anochece', 'actor', 'agujero', 'albañil', 'alce', 'alegre', 'alto', 'amargo', 'amplio', 'amarillo', 'y', 'o', 'no', 'algún', 'blicket', 'bliscea', 'brilca', 'brunza', 'dernea', 'drifla', 'flajuf', 'flakle', 'jufmoq', 'jufzyl', 'brispado', 'claribundo', 'dormínico', 'florido', 'flexivo', 'ningún']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10650, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10645</th>\n",
       "      <td>algún alce amanece o anochece</td>\n",
       "      <td>todo alce amanece o anochece</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10646</th>\n",
       "      <td>algún alce anochece o acuerda</td>\n",
       "      <td>todo alce anochece o acuerda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>algún alce anochece o aguanta</td>\n",
       "      <td>todo alce anochece o aguanta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>algún alce anochece o ama</td>\n",
       "      <td>todo alce anochece o ama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>algún alce anochece o amanece</td>\n",
       "      <td>todo alce anochece o amanece</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sentence 1                     sentence 2  relation\n",
       "10645  algún alce amanece o anochece    todo alce amanece o anochece         0\n",
       "10646  algún alce anochece o acuerda    todo alce anochece o acuerda         0\n",
       "10647  algún alce anochece o aguanta    todo alce anochece o aguanta         0\n",
       "10648      algún alce anochece o ama        todo alce anochece o ama         0\n",
       "10649  algún alce anochece o amanece    todo alce anochece o amanece         0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = pd.read_csv(raw_dataset, names=['sentence 1', 'sentence 2', 'relation'])\n",
    "# sentences_df = pd.read_csv(jabberwockie_dataset, names=['sentence 1', 'sentence 2', 'relation'])\n",
    "print(sentences_df.shape)\n",
    "sentences_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "      <th>tokens sentence 1</th>\n",
       "      <th>tokens sentence 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo abuelo acuerda</td>\n",
       "      <td>algún abuelo no acuerda</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[algún, abuelo, no, acuerda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>todo abuelo aguanta</td>\n",
       "      <td>algún abuelo no aguanta</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[algún, abuelo, no, aguanta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo abuelo ama</td>\n",
       "      <td>algún abuelo no ama</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, ama]</td>\n",
       "      <td>[algún, abuelo, no, ama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>todo abuelo amanece</td>\n",
       "      <td>algún abuelo no amanece</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, amanece]</td>\n",
       "      <td>[algún, abuelo, no, amanece]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todo abuelo anochece</td>\n",
       "      <td>algún abuelo no anochece</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[algún, abuelo, no, anochece]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sentence 1                 sentence 2  relation  \\\n",
       "0   todo abuelo acuerda     algún abuelo no acuerda         1   \n",
       "1   todo abuelo aguanta     algún abuelo no aguanta         1   \n",
       "2       todo abuelo ama         algún abuelo no ama         1   \n",
       "3   todo abuelo amanece     algún abuelo no amanece         1   \n",
       "4  todo abuelo anochece    algún abuelo no anochece         1   \n",
       "\n",
       "          tokens sentence 1              tokens sentence 2  \n",
       "0   [todo, abuelo, acuerda]   [algún, abuelo, no, acuerda]  \n",
       "1   [todo, abuelo, aguanta]   [algún, abuelo, no, aguanta]  \n",
       "2       [todo, abuelo, ama]       [algún, abuelo, no, ama]  \n",
       "3   [todo, abuelo, amanece]   [algún, abuelo, no, amanece]  \n",
       "4  [todo, abuelo, anochece]  [algún, abuelo, no, anochece]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['tokens sentence 1'] = sentences_df['sentence 1'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "sentences_df['tokens sentence 2'] = sentences_df['sentence 2'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_true_false(p):\n",
    "    # Create a Bernoulli distribution with probability p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
    "    # Sample from this distribution and convert 1 to True and 0 to False\n",
    "    return bernoulli_dist.sample().item() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    # Decide whether to mask this token (20% chance)\n",
    "    mask = bernoulli_true_false(0.2)\n",
    "\n",
    "    # If mask is False, immediately return with '[PAD]' label\n",
    "    if not mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # If mask is True, proceed with further operations\n",
    "    # Randomly decide on an operation (50% chance each)\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    # Case 1: If mask, random_opp, and random_swich are True\n",
    "    if mask and random_opp and random_swich:\n",
    "        # Replace the token with '[MASK]' and set label to a random token\n",
    "        mask_label = tokenizer.decode(torch.randint(0, VOCAB_SIZE, (1,)))[0]\n",
    "        token_ = '[MASK]'\n",
    "\n",
    "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
    "    elif mask and random_opp and not random_swich:\n",
    "        # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    # Case 3: If mask is True, but random_opp is False\n",
    "    else:\n",
    "        # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] abuelo \t Actual token *abuelo* is masked with '[MASK]'\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "[MASK] blicket \t Actual token *abuelo* is replaced with random token #blicket#\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n"
     ]
    }
   ],
   "source": [
    "# Test Masking\n",
    "torch.manual_seed(100)\n",
    "for l in range(10):\n",
    "  token=\"abuelo\"\n",
    "  token_,label=Masking(token)\n",
    "  if token==token_ and label==\"[PAD]\":\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is left unchanged\")\n",
    "  elif token_==\"[MASK]\" and label==token:\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is masked with '{token_}'\")\n",
    "  else:\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is replaced with random token #{label}#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
    "    \"\"\"\n",
    "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
    "\n",
    "    \"\"\"\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    raw_tokens = []  # List to store raw tokens if needed\n",
    "\n",
    "    for token in tokens:\n",
    "        # Apply BERT's MLM masking strategy to the token\n",
    "        masked_token, mask_label = Masking(token)\n",
    "\n",
    "        # Append the processed token and its label to the current sentence and label list\n",
    "        bert_input.append(masked_token)\n",
    "        bert_label.append(mask_label)\n",
    "\n",
    "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens.append(token)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without raw tokens: \t  \n",
      " \t original_input is: \t  algún abuelo alegre no ama \n",
      " \t bert_input is: \t  ['[MASK]', 'abuelo', 'alegre', 'no', 'ama'] \n",
      " \t bert_label is: \t  ['algún', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "With raw tokens: \t  \n",
      " \t original_input is: \t  algún abuelo alegre no ama \n",
      " \t bert_input is: \t  ['algún', 'abuelo', 'alegre', 'no', 'ama'] \n",
      " \t bert_label is: \t  ['[PAD]', '[PAD]', '[PAD]', 'no', '[PAD]'] \n",
      " \t raw_tokens_list is: \t  ['algún', 'abuelo', 'alegre', 'no', 'ama']\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "# original_input=\"The sun sets behind the distant mountains.\"\n",
    "original_input = \"algún abuelo alegre no ama\"\n",
    "tokens=tokenizer.encode(original_input).tokens\n",
    "bert_input, bert_label= prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "print(\"Without raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label)\n",
    "print(\"-\"*200)\n",
    "torch.manual_seed(200)\n",
    "bert_input, bert_label, raw_tokens_list = prepare_for_mlm(tokens, include_raw_tokens=True)\n",
    "print(\"With raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label,\"\\n \\t raw_tokens_list is: \\t \", raw_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "      <th>tokens sentence 1</th>\n",
       "      <th>tokens sentence 2</th>\n",
       "      <th>bert_input 1</th>\n",
       "      <th>bert_label 1</th>\n",
       "      <th>bert_input 2</th>\n",
       "      <th>bert_label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo abuelo acuerda</td>\n",
       "      <td>algún abuelo no acuerda</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[algún, abuelo, no, acuerda]</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, no, acuerda]</td>\n",
       "      <td>[[PAD], [PAD], [PAD], [PAD]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>todo abuelo aguanta</td>\n",
       "      <td>algún abuelo no aguanta</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[algún, abuelo, no, aguanta]</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, [MASK], [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], no, [MASK]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo abuelo ama</td>\n",
       "      <td>algún abuelo no ama</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, ama]</td>\n",
       "      <td>[algún, abuelo, no, ama]</td>\n",
       "      <td>[todo, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], ama]</td>\n",
       "      <td>[algún, abuelo, no, ama]</td>\n",
       "      <td>[[PAD], [PAD], [PAD], [PAD]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>todo abuelo amanece</td>\n",
       "      <td>algún abuelo no amanece</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, amanece]</td>\n",
       "      <td>[algún, abuelo, no, amanece]</td>\n",
       "      <td>[todo, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], amanece]</td>\n",
       "      <td>[algún, abuelo, no, amanece]</td>\n",
       "      <td>[[PAD], [PAD], no, [PAD]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todo abuelo anochece</td>\n",
       "      <td>algún abuelo no anochece</td>\n",
       "      <td>1</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[algún, abuelo, no, anochece]</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, [MASK], anochece]</td>\n",
       "      <td>[[PAD], [PAD], no, [PAD]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sentence 1                 sentence 2  relation  \\\n",
       "0   todo abuelo acuerda     algún abuelo no acuerda         1   \n",
       "1   todo abuelo aguanta     algún abuelo no aguanta         1   \n",
       "2       todo abuelo ama         algún abuelo no ama         1   \n",
       "3   todo abuelo amanece     algún abuelo no amanece         1   \n",
       "4  todo abuelo anochece    algún abuelo no anochece         1   \n",
       "\n",
       "          tokens sentence 1              tokens sentence 2  \\\n",
       "0   [todo, abuelo, acuerda]   [algún, abuelo, no, acuerda]   \n",
       "1   [todo, abuelo, aguanta]   [algún, abuelo, no, aguanta]   \n",
       "2       [todo, abuelo, ama]       [algún, abuelo, no, ama]   \n",
       "3   [todo, abuelo, amanece]   [algún, abuelo, no, amanece]   \n",
       "4  [todo, abuelo, anochece]  [algún, abuelo, no, anochece]   \n",
       "\n",
       "               bert_input 1             bert_label 1  \\\n",
       "0   [todo, abuelo, acuerda]    [[PAD], [PAD], [PAD]]   \n",
       "1   [todo, abuelo, aguanta]    [[PAD], [PAD], [PAD]]   \n",
       "2    [todo, abuelo, [MASK]]      [[PAD], [PAD], ama]   \n",
       "3    [todo, abuelo, [MASK]]  [[PAD], [PAD], amanece]   \n",
       "4  [todo, abuelo, anochece]    [[PAD], [PAD], [PAD]]   \n",
       "\n",
       "                        bert_input 2                  bert_label 2  \n",
       "0       [algún, abuelo, no, acuerda]  [[PAD], [PAD], [PAD], [PAD]]  \n",
       "1    [algún, abuelo, [MASK], [MASK]]    [[PAD], [PAD], no, [MASK]]  \n",
       "2           [algún, abuelo, no, ama]  [[PAD], [PAD], [PAD], [PAD]]  \n",
       "3       [algún, abuelo, no, amanece]     [[PAD], [PAD], no, [PAD]]  \n",
       "4  [algún, abuelo, [MASK], anochece]     [[PAD], [PAD], no, [PAD]]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['bert_input_label 1'] = sentences_df['tokens sentence 1'].apply(lambda x: prepare_for_mlm(x))\n",
    "sentences_df['bert_input 1'] = sentences_df['bert_input_label 1'].apply(lambda x: x[0])\n",
    "sentences_df['bert_label 1'] = sentences_df['bert_input_label 1'].apply(lambda x: x[1])\n",
    "sentences_df['bert_input_label 2'] = sentences_df['tokens sentence 2'].apply(lambda x: prepare_for_mlm(x))\n",
    "sentences_df['bert_input 2'] = sentences_df['bert_input_label 2'].apply(lambda x: x[0])\n",
    "sentences_df['bert_label 2'] = sentences_df['bert_input_label 2'].apply(lambda x: x[1])\n",
    "del sentences_df['bert_input_label 1']\n",
    "del sentences_df['bert_input_label 2']\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_pair = sentences_df[['bert_input 1', 'bert_input 2']].values.tolist()\n",
    "input_masked_labels_pair = sentences_df[['bert_label 1', 'bert_label 2']].values.tolist()\n",
    "relations = sentences_df['relation'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_nsp(input_sentences_pair, input_masked_labels_pair, relations):\n",
    "    \"\"\"\n",
    "    Prepares data for understanding logical relationship.\n",
    "\n",
    "    Args:\n",
    "    input_sentences (list): List of tokenized sentences.\n",
    "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
    "\n",
    "    Returns:\n",
    "    bert_input (list): List of sentence pairs for BERT input.\n",
    "    bert_label (list): List of masked labels for the sentence pairs.\n",
    "    is_next (list): Binary label list where 1 indicates 'logical relationship' and 0 indicates 'not logical relationship'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "    if len(input_sentences_pair) != len(input_masked_labels_pair):\n",
    "        raise ValueError(\"Both lists, input_sentences_pair and input_masked_labels_pair, must have the same number of items.\")\n",
    "    if len(input_sentences_pair) != len(relations):\n",
    "        raise ValueError(\"Both lists, input_sentences_pair and relations, must have the same number of items.\")\n",
    "\n",
    "    bert_input = []\n",
    "    bert_label = []\n",
    "    is_next = []\n",
    "\n",
    "    for sentence_pair, masked_pair, relation in zip(input_sentences_pair, input_masked_labels_pair, relations):\n",
    "        # append list and add  '[CLS]' and  '[SEP]' tokens\n",
    "        bert_input.append([['[CLS]'] + sentence_pair[0] + ['[SEP]'], sentence_pair[1] + ['[SEP]']])\n",
    "        bert_label.append([['[PAD]'] + masked_pair[0] + ['[PAD]'], masked_pair[1]+ ['[PAD]']])\n",
    "        is_next.append(relation)  # Label 1 indicates these sentences have the required logical relationship\n",
    "\n",
    "    return bert_input, bert_label, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, bert_labels, is_nexts = process_for_nsp(input_sentences_pair, input_masked_labels_pair, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[CLS]', 'todo', 'abuelo', 'acuerda', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'no', 'acuerda', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'aguanta', '[SEP]'],\n",
       "  ['algún', 'abuelo', '[MASK]', '[MASK]', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', '[MASK]', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'no', 'ama', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', '[MASK]', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'no', 'amanece', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'anochece', '[SEP]'],\n",
       "  ['algún', 'abuelo', '[MASK]', 'anochece', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'actor', 'acuerda', '[SEP]'],\n",
       "  ['[MASK]', 'actor', 'no', 'acuerda', '[SEP]']],\n",
       " [['[CLS]', 'todo', '[MASK]', 'aguanta', '[SEP]'],\n",
       "  ['algún', 'actor', 'no', '[MASK]', '[SEP]']],\n",
       " [['[CLS]', '[MASK]', 'actor', 'ama', '[SEP]'],\n",
       "  ['[MASK]', 'actor', 'no', 'ama', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'actor', 'amanece', '[SEP]'],\n",
       "  ['algún', '[MASK]', 'no', 'amanece', '[SEP]']],\n",
       " [['[CLS]', 'todo', '[MASK]', '[MASK]', '[SEP]'],\n",
       "  ['algún', '[MASK]', 'no', 'anochece', '[SEP]']]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #transform tokens to vocab indices\n",
    "    tokens_to_index=lambda tokens: [tokenizer.stoi[token] for token in tokens]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        #convert to tensors\n",
    "        if to_tensor:\n",
    "\n",
    "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "            # bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
    "            # bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            # segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
    "            bert_inputs_final.append(tokens_to_index(flatten(bert_input_padded)))\n",
    "            bert_labels_final.append(tokens_to_index(flatten(bert_label_padded)))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "        else:\n",
    "          # Flatten the padded inputs and labels\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            bert_labels_final.append(flatten(bert_label_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 6, 7, 3, 24, 6, 23, 7, 3],\n",
       " [2, 5, 6, 8, 3, 24, 6, 4, 4, 3],\n",
       " [2, 5, 6, 4, 3, 24, 6, 23, 9, 3],\n",
       " [2, 5, 6, 4, 3, 24, 6, 23, 10, 3],\n",
       " [2, 5, 6, 11, 3, 24, 6, 4, 11, 3],\n",
       " [2, 5, 12, 7, 3, 4, 12, 23, 7, 3],\n",
       " [2, 5, 4, 8, 3, 24, 12, 23, 4, 3],\n",
       " [2, 4, 12, 9, 3, 4, 12, 23, 9, 3],\n",
       " [2, 5, 12, 10, 3, 24, 4, 23, 10, 3],\n",
       " [2, 5, 4, 4, 3, 24, 4, 23, 11, 3]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 23, 4, 1],\n",
       " [1, 1, 1, 9, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 10, 1, 1, 1, 23, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 23, 1, 1],\n",
       " [1, 1, 1, 1, 1, 24, 1, 1, 1, 1],\n",
       " [1, 1, 12, 1, 1, 1, 1, 1, 8, 1],\n",
       " [1, 5, 1, 1, 1, 24, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 12, 1, 1, 1],\n",
       " [1, 1, 12, 30, 1, 1, 12, 1, 1, 1]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_labels_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 2]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'bert_input': bert_inputs_final,\n",
    "    'bert_label': bert_labels_final,\n",
    "    'segment_label': segment_labels_final,\n",
    "    'relation': is_nexts_final\n",
    "})\n",
    "\n",
    "df_final.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
