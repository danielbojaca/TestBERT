{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bert import BERT\n",
    "from utils_vocab import BasicTokenizer, BERTDatasetNoLabels, evaluate\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "tokenizer_file = 'tokenizer_5.pkl'\n",
    "# tokenizer_file = 'tokenizer_10.pkl'\n",
    "# tokenizer_file = 'tokenizer_15.pkl'\n",
    "# tokenizer_file = 'tokenizer_20.pkl'\n",
    "\n",
    "raw_dataset = 'implicacion_inversa_5.csv'\n",
    "# raw_dataset = 'implicacion_inversa_10.csv'\n",
    "# raw_dataset = 'implicacion_inversa_15.csv'\n",
    "# raw_dataset = 'implicacion_inversa_20.csv'\n",
    "\n",
    "path_model = 'implicacion_inversa_5.pt'\n",
    "# path_model = 'implicacion_inversa_10.pt'\n",
    "# path_model = 'implicacion_inversa_15.pt'\n",
    "# path_model = 'implicacion_inversa_20.pt'\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size: 41\n"
     ]
    }
   ],
   "source": [
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "PAD_IDX = 1\n",
    "CLS_IDX = 2\n",
    "SEP_IDX = 3\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer.create_using_stoi(simple_tokenizer, special_symbols, tokenizer_file)\n",
    "print('vocabulary_size:', tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar datos y crear dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18899, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(raw_dataset)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts, is_consistents, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final, is_consistents_final = [], [], [], [], []\n",
    "\n",
    "    for sentence1, sentence2, is_next, is_consistent in zip(sentences1, sentences2, is_nexts, is_consistents):\n",
    "        # Tokenize each sentence\n",
    "        tokens1 = tokenizer.encode(sentence1).ids\n",
    "        tokens2 = tokenizer.encode(sentence2).ids\n",
    "        bert_input = ([CLS_IDX] + tokens1 + [SEP_IDX], tokens2 + [SEP_IDX])\n",
    "\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        bert_inputs_final.append(flatten(bert_input_padded))\n",
    "        segment_labels_final.append(flatten(segment_label_padded))\n",
    "        is_nexts_final.append(is_next)\n",
    "        is_consistents_final.append(is_consistent)\n",
    "\n",
    "    return bert_inputs_final, segment_labels_final, is_nexts_final, is_consistents_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df.iloc[:, 0]\n",
    "sentences2 = df.iloc[:, 1]\n",
    "is_nexts = df.iloc[:, 2]\n",
    "is_consistent = df.iloc[:, 3]\n",
    "bert_inputs_final, segment_labels_final, is_nexts_final, is_consistent_final = direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts, is_consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_final.shape=(18899, 4)\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'BERT Input': bert_inputs_final,\n",
    "    'Segment Label': segment_labels_final,\n",
    "    'Is Next': is_nexts_final,\n",
    "    'Is Consistent': is_consistent_final\n",
    "})\n",
    "print(f'{df_final.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BERTDatasetNoLabels(df_final)\n",
    "bert_inputs, segment_labels, is_nexts, is_consistents = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, segment_labels_batch, is_nexts_batch, is_consistents_batch = [], [], [], []\n",
    "\n",
    "    for bert_input, segment_label, is_next, is_consistent in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(torch.tensor(is_next, dtype=torch.long))\n",
    "        is_consistents_batch.append(torch.tensor(is_consistent, dtype=torch.long))\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch)\n",
    "    is_consistents_batch = torch.tensor(is_consistents_batch)\n",
    "\n",
    "    return bert_inputs_final.to(device), segment_labels_final.to(device), is_nexts_batch.to(device), is_consistents_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts, is_consistents = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert_embedding): BERTEmbedding(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(41, 16)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (segment_embedding): Embedding(3, 16)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (nextsentenceprediction): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (masked_language): Linear(in_features=16, out_features=41, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = tokenizer.get_vocab_size()  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 4  # Number of Transformer layers\n",
    "initial_heads = 4\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_model, weights_only=True,map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts, is_consistents = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 24, 18, 13, 21, 11, 3, 5, 13, 21, 24, 11, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 21, 7, 17, 11, 3, 1, 1, 24, 5, 14, 21, 24, 7, 16, 24, 11, 3],\n",
       " [2, 24, 18, 13, 19, 9, 3, 18, 13, 19, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 9, 17, 10, 3, 18, 12, 23, 9, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 13, 10, 17, 7, 3, 1, 5, 13, 24, 10, 16, 24, 7, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 6, 9, 16, 11, 3, 18, 6, 24, 9, 17, 24, 11, 3, 1, 1, 1, 1],\n",
       " [2, 5, 14, 20, 7, 17, 10, 3, 1, 18, 14, 20, 24, 7, 16, 24, 10, 3, 1, 1],\n",
       " [2, 24, 18, 13, 20, 8, 3, 18, 13, 20, 8, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 13, 23, 11, 3, 5, 13, 23, 24, 11, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 20, 24, 9, 16, 24, 8, 3, 25, 12, 20, 9, 17, 8, 3, 1, 1, 1],\n",
       " [2, 24, 18, 13, 19, 10, 3, 18, 13, 19, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 15, 9, 16, 11, 3, 18, 15, 22, 9, 16, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 14, 22, 11, 3, 18, 14, 22, 24, 11, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 13, 20, 24, 11, 16, 24, 9, 3, 25, 13, 20, 11, 17, 9, 3, 1, 1, 1],\n",
       " [2, 18, 13, 22, 8, 16, 7, 3, 1, 1, 24, 5, 13, 22, 24, 8, 17, 24, 7, 3],\n",
       " [2, 24, 18, 6, 21, 9, 16, 11, 3, 5, 6, 21, 24, 9, 17, 24, 11, 3, 1, 1],\n",
       " [2, 5, 6, 20, 8, 16, 10, 3, 5, 6, 8, 16, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 13, 19, 24, 7, 17, 24, 8, 3, 25, 13, 19, 7, 16, 8, 3, 1, 1, 1],\n",
       " [2, 24, 5, 13, 20, 8, 16, 7, 3, 5, 13, 20, 8, 16, 7, 3, 1, 1, 1, 1],\n",
       " [2, 18, 6, 20, 7, 17, 10, 3, 1, 5, 6, 20, 24, 7, 16, 24, 10, 3, 1, 1],\n",
       " [2, 5, 12, 20, 7, 17, 9, 3, 1, 18, 12, 20, 24, 7, 16, 24, 9, 3, 1, 1],\n",
       " [2, 18, 14, 22, 24, 9, 17, 24, 8, 3, 25, 14, 22, 9, 16, 8, 3, 1, 1, 1],\n",
       " [2, 24, 18, 12, 22, 9, 16, 8, 3, 5, 12, 22, 24, 9, 17, 24, 8, 3, 1, 1],\n",
       " [2, 5, 14, 23, 11, 17, 10, 3, 1, 18, 14, 23, 24, 11, 16, 24, 10, 3, 1, 1],\n",
       " [2, 5, 6, 11, 16, 9, 3, 5, 6, 21, 11, 16, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 20, 24, 7, 17, 24, 10, 3, 25, 14, 20, 7, 16, 10, 3, 1, 1, 1],\n",
       " [2, 5, 14, 21, 9, 17, 7, 3, 1, 18, 14, 21, 24, 9, 16, 24, 7, 3, 1, 1],\n",
       " [2, 24, 5, 6, 19, 11, 3, 25, 6, 19, 11, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 14, 9, 16, 8, 3, 5, 14, 21, 9, 16, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 22, 24, 9, 16, 24, 7, 3, 25, 14, 22, 9, 17, 7, 3, 1, 1, 1],\n",
       " [2, 18, 12, 20, 9, 17, 7, 3, 1, 5, 12, 20, 24, 9, 16, 24, 7, 3, 1, 1],\n",
       " [2, 5, 12, 22, 7, 16, 9, 3, 1, 18, 12, 22, 24, 7, 17, 24, 9, 3, 1, 1],\n",
       " [2, 24, 5, 6, 21, 9, 16, 8, 3, 25, 6, 21, 9, 16, 8, 3, 1, 1, 1, 1],\n",
       " [2, 24, 18, 6, 22, 7, 16, 9, 3, 18, 6, 22, 7, 16, 9, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 15, 20, 8, 17, 11, 3, 5, 15, 20, 8, 17, 11, 3, 1, 1, 1, 1],\n",
       " [2, 24, 18, 14, 22, 7, 16, 8, 3, 5, 14, 22, 24, 7, 17, 24, 8, 3, 1, 1],\n",
       " [2, 5, 13, 20, 7, 17, 8, 3, 5, 13, 7, 17, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 15, 22, 8, 17, 9, 3, 18, 15, 22, 24, 8, 16, 24, 9, 3, 1, 1],\n",
       " [2, 5, 13, 23, 8, 16, 10, 3, 1, 18, 13, 23, 24, 8, 17, 24, 10, 3, 1, 1],\n",
       " [2, 24, 5, 14, 8, 17, 10, 3, 25, 14, 8, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 14, 7, 17, 11, 3, 18, 14, 7, 17, 11, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 12, 8, 17, 11, 3, 1, 18, 12, 24, 8, 16, 24, 11, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 24, 8, 16, 24, 11, 3, 18, 13, 8, 17, 11, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 20, 9, 17, 8, 3, 25, 13, 20, 9, 17, 8, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 6, 21, 8, 17, 10, 3, 5, 6, 21, 8, 17, 10, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 19, 10, 16, 7, 3, 25, 13, 19, 10, 16, 7, 3, 1, 1, 1, 1],\n",
       " [2, 18, 6, 21, 24, 9, 17, 24, 8, 3, 25, 6, 21, 9, 16, 8, 3, 1, 1, 1],\n",
       " [2, 5, 6, 21, 7, 17, 10, 3, 1, 18, 6, 21, 24, 7, 16, 24, 10, 3, 1, 1],\n",
       " [2, 24, 5, 6, 22, 9, 16, 7, 3, 18, 6, 22, 24, 9, 17, 24, 7, 3, 1, 1],\n",
       " [2, 18, 14, 23, 9, 16, 11, 3, 1, 5, 14, 23, 24, 9, 17, 24, 11, 3, 1, 1],\n",
       " [2, 5, 12, 21, 9, 16, 10, 3, 5, 12, 9, 16, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 18, 6, 23, 8, 16, 7, 3, 5, 6, 23, 24, 8, 17, 24, 7, 3, 1, 1],\n",
       " [2, 24, 5, 13, 24, 11, 16, 24, 10, 3, 18, 13, 11, 17, 10, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 14, 22, 9, 17, 8, 3, 18, 14, 22, 24, 9, 16, 24, 8, 3, 1, 1],\n",
       " [2, 24, 18, 6, 22, 11, 17, 8, 3, 5, 6, 22, 24, 11, 16, 24, 8, 3, 1, 1],\n",
       " [2, 5, 6, 21, 11, 17, 7, 3, 1, 18, 6, 21, 24, 11, 16, 24, 7, 3, 1, 1],\n",
       " [2, 24, 18, 14, 22, 10, 16, 8, 3, 5, 14, 22, 24, 10, 17, 24, 8, 3, 1, 1],\n",
       " [2, 5, 12, 7, 16, 8, 3, 18, 12, 7, 16, 8, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 7, 17, 10, 3, 18, 14, 22, 7, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 15, 19, 9, 17, 10, 3, 1, 5, 15, 19, 24, 9, 16, 24, 10, 3, 1, 1],\n",
       " [2, 5, 15, 19, 11, 17, 9, 3, 1, 18, 15, 19, 24, 11, 16, 24, 9, 3, 1, 1],\n",
       " [2, 18, 13, 19, 11, 17, 10, 3, 1, 5, 13, 19, 24, 11, 16, 24, 10, 3, 1, 1],\n",
       " [2, 24, 5, 15, 22, 8, 16, 10, 3, 25, 15, 22, 8, 16, 10, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 20, 7, 16, 10, 3, 18, 13, 20, 24, 7, 17, 24, 10, 3, 1, 1],\n",
       " [2, 5, 15, 20, 10, 16, 9, 3, 5, 15, 10, 16, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 20, 10, 3, 5, 14, 20, 24, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 23, 9, 16, 8, 3, 5, 13, 23, 9, 16, 8, 3, 1, 1, 1, 1],\n",
       " [2, 24, 18, 13, 23, 9, 16, 8, 3, 18, 13, 23, 9, 16, 8, 3, 1, 1, 1, 1],\n",
       " [2, 5, 12, 7, 17, 11, 3, 1, 18, 12, 24, 7, 16, 24, 11, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 15, 19, 9, 16, 10, 3, 5, 15, 19, 9, 16, 10, 3, 1, 1, 1, 1],\n",
       " [2, 18, 13, 19, 9, 17, 10, 3, 18, 13, 9, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 15, 22, 7, 3, 5, 15, 22, 24, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 15, 19, 10, 16, 9, 3, 5, 15, 10, 16, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 15, 8, 16, 10, 3, 5, 15, 22, 8, 16, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 18, 14, 23, 7, 17, 9, 3, 5, 14, 23, 24, 7, 16, 24, 9, 3, 1, 1],\n",
       " [2, 18, 13, 7, 3, 18, 13, 19, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 12, 11, 17, 9, 3, 18, 12, 24, 11, 16, 24, 9, 3, 1, 1, 1, 1],\n",
       " [2, 24, 18, 12, 20, 10, 17, 11, 3, 18, 12, 20, 10, 17, 11, 3, 1, 1, 1, 1],\n",
       " [2, 5, 13, 22, 7, 17, 8, 3, 1, 18, 13, 22, 24, 7, 16, 24, 8, 3, 1, 1],\n",
       " [2, 5, 14, 19, 8, 16, 11, 3, 5, 14, 8, 16, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 23, 8, 16, 9, 3, 1, 1, 24, 5, 14, 23, 24, 8, 17, 24, 9, 3],\n",
       " [2, 18, 15, 8, 16, 9, 3, 18, 15, 19, 8, 16, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 13, 7, 3, 5, 13, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 6, 20, 9, 17, 8, 3, 18, 6, 20, 24, 9, 16, 24, 8, 3, 1, 1],\n",
       " [2, 5, 14, 21, 7, 17, 9, 3, 1, 18, 14, 21, 24, 7, 16, 24, 9, 3, 1, 1],\n",
       " [2, 24, 5, 12, 20, 9, 17, 7, 3, 18, 12, 20, 24, 9, 16, 24, 7, 3, 1, 1],\n",
       " [2, 5, 15, 19, 9, 3, 25, 15, 19, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 6, 21, 10, 16, 8, 3, 5, 6, 21, 10, 16, 8, 3, 1, 1, 1, 1],\n",
       " [2, 18, 15, 10, 16, 8, 3, 18, 15, 23, 10, 16, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 14, 21, 7, 17, 8, 3, 25, 14, 21, 7, 17, 8, 3, 1, 1, 1, 1],\n",
       " [2, 5, 15, 19, 9, 16, 11, 3, 5, 15, 9, 16, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 14, 21, 24, 10, 3, 25, 14, 21, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 6, 21, 7, 3, 18, 6, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 14, 7, 3, 5, 14, 23, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 19, 7, 16, 8, 3, 1, 18, 6, 19, 24, 7, 17, 24, 8, 3, 1, 1],\n",
       " [2, 24, 5, 13, 20, 10, 16, 11, 3, 5, 13, 20, 10, 16, 11, 3, 1, 1, 1, 1],\n",
       " [2, 24, 5, 6, 23, 10, 16, 11, 3, 25, 6, 23, 10, 16, 11, 3, 1, 1, 1, 1],\n",
       " [2, 18, 13, 22, 10, 17, 8, 3, 18, 13, 10, 17, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 12, 10, 17, 9, 3, 5, 12, 19, 10, 17, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 6, 20, 24, 10, 16, 24, 9, 3, 25, 6, 20, 10, 17, 9, 3, 1, 1, 1],\n",
       " [2, 18, 14, 11, 17, 10, 3, 18, 14, 22, 11, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 15, 22, 11, 17, 7, 3, 1, 18, 15, 22, 24, 11, 16, 24, 7, 3, 1, 1],\n",
       " [2, 5, 12, 22, 7, 16, 11, 3, 1, 18, 12, 22, 24, 7, 17, 24, 11, 3, 1, 1],\n",
       " [2, 24, 5, 6, 23, 9, 17, 10, 3, 18, 6, 23, 24, 9, 16, 24, 10, 3, 1, 1],\n",
       " [2, 18, 14, 10, 17, 7, 3, 18, 14, 23, 10, 17, 7, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 12, 24, 11, 17, 24, 9, 3, 25, 12, 11, 16, 9, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 15, 8, 17, 7, 3, 5, 15, 20, 8, 17, 7, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 23, 10, 17, 8, 3, 18, 12, 10, 17, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 14, 20, 11, 17, 7, 3, 1, 18, 14, 20, 24, 11, 16, 24, 7, 3, 1, 1],\n",
       " [2, 18, 6, 9, 17, 7, 3, 18, 6, 21, 9, 17, 7, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 24, 18, 15, 19, 9, 16, 7, 3, 5, 15, 19, 24, 9, 17, 24, 7, 3, 1, 1],\n",
       " [2, 5, 12, 24, 10, 16, 24, 7, 3, 25, 12, 10, 17, 7, 3, 1, 1, 1, 1, 1],\n",
       " [2, 24, 5, 12, 20, 10, 17, 8, 3, 18, 12, 20, 24, 10, 16, 24, 8, 3, 1, 1],\n",
       " [2, 24, 18, 13, 22, 10, 16, 9, 3, 18, 13, 22, 10, 16, 9, 3, 1, 1, 1, 1],\n",
       " [2, 18, 15, 21, 24, 10, 16, 24, 8, 3, 25, 15, 21, 10, 17, 8, 3, 1, 1, 1],\n",
       " [2, 24, 5, 12, 22, 7, 16, 8, 3, 18, 12, 22, 24, 7, 17, 24, 8, 3, 1, 1],\n",
       " [2, 24, 5, 12, 22, 10, 16, 9, 3, 5, 12, 22, 10, 16, 9, 3, 1, 1, 1, 1],\n",
       " [2, 5, 13, 20, 8, 17, 9, 3, 5, 13, 8, 17, 9, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 15, 22, 11, 17, 10, 3, 18, 15, 11, 17, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 21, 11, 17, 8, 3, 18, 12, 11, 17, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 15, 8, 16, 10, 3, 18, 15, 22, 8, 16, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 23, 24, 7, 16, 24, 8, 3, 25, 12, 23, 7, 17, 8, 3, 1, 1, 1],\n",
       " [2, 24, 18, 6, 23, 10, 3, 18, 6, 23, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 18, 12, 9, 16, 8, 3, 5, 12, 9, 16, 8, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 15, 7, 16, 9, 3, 18, 15, 7, 16, 9, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 6, 22, 8, 16, 7, 3, 1, 18, 6, 22, 24, 8, 17, 24, 7, 3, 1, 1],\n",
       " [2, 24, 18, 13, 19, 11, 17, 10, 3, 18, 13, 19, 11, 17, 10, 3, 1, 1, 1, 1],\n",
       " [2, 25, 6, 9, 17, 7, 3, 1, 5, 6, 24, 9, 16, 24, 7, 3, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.cpu().numpy().transpose(1, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    next_sentence_prediction, masked_language = model(bert_inputs.to(device), segment_labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_sentences(bert_input):\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0] + pair[1]\n",
    "\n",
    "    tokens = bert_input.cpu().numpy().tolist()\n",
    "    assert(len(tokens) % 2 == 0), len(tokens)\n",
    "    try:\n",
    "        len_sents = tokens.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(tokens)\n",
    "        print(tokenizer.decode(tokens))\n",
    "        raise Exception(e)\n",
    "    first = tokens[1:len_sents - 1]\n",
    "    second_ = tokens[len_sents:]\n",
    "    try:\n",
    "        sep_in_second = second_.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(second_)\n",
    "        print(tokenizer.decode(second_))\n",
    "        raise Exception(e)\n",
    "    second = second_[:sep_in_second]\n",
    "    bert_input = ([CLS_IDX] + second + [SEP_IDX], first + [SEP_IDX])\n",
    "    # Create segment labels for each pair of sentences\n",
    "    segment_label = ([1] * len(bert_input[0]), [2] * len(bert_input[1]))\n",
    "    # Zero-pad the bert_input and bert_label and segment_label\n",
    "    bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "    bert_input_padded = torch.tensor(bert_input_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "    segment_label_padded = torch.tensor(segment_label_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    return bert_input_padded, segment_label_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(input, segment_label, is_next, device):\n",
    "    with torch.no_grad():\n",
    "        model_prediction, _ = model(input.to(device), segment_label.to(device))\n",
    "    logits = torch.softmax(model_prediction, dim=1)\n",
    "    prediction = torch.argmax(logits, dim=1)     \n",
    "    correct = prediction == is_next\n",
    "    return correct, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc38ce7dca8409f84b4c59e641bc39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5916296234169253\n",
      "Accuracy: 0.5660427807486631\n"
     ]
    }
   ],
   "source": [
    "test = list()\n",
    "golden = list()\n",
    "for bert_inputs, segment_labels, is_nexts, is_consistents in tqdm(dataloader):\n",
    "    corrects,_ = get_prediction(\n",
    "        input=bert_inputs,\n",
    "        segment_label=segment_labels,\n",
    "        is_next=is_nexts,\n",
    "        device=device\n",
    "    )\n",
    "    for idx, correct in enumerate(corrects):\n",
    "        if correct:\n",
    "            bert_input = bert_inputs.transpose(1,0)[idx]\n",
    "            bert_input_reversed, segment_label = invert_sentences(bert_input)\n",
    "            check, prediction = get_prediction(\n",
    "                input=bert_input_reversed, \n",
    "                segment_label=segment_label,\n",
    "                is_next=is_consistents[idx],\n",
    "                device=device\n",
    "            )\n",
    "            test.append(prediction.cpu().item())\n",
    "            golden.append(is_consistents[idx].cpu())\n",
    "acc = accuracy_score(golden, test)\n",
    "f1 = f1_score(golden, test)    \n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
