{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bert import BERT\n",
    "from utils_vocab import BasicTokenizer, BERTDatasetNoLabels, evaluate\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# raw_dataset = 'implicacion_5.csv'\n",
    "# raw_dataset = 'implicacion_10.csv'\n",
    "# raw_dataset = 'implicacion_15.csv'\n",
    "raw_dataset = 'implicacion_20.csv'\n",
    "\n",
    "# tokenizer_file = 'tokenizer_5.pkl'\n",
    "# tokenizer_file = 'tokenizer_10.pkl'\n",
    "# tokenizer_file = 'tokenizer_15.pkl'\n",
    "tokenizer_file = 'tokenizer_20.pkl'\n",
    "\n",
    "# path_model = 'implicacion_5.pt'\n",
    "# path_model = 'implicacion_10.pt'\n",
    "# path_model = 'implicacion_15.pt'\n",
    "path_model = 'implicacion_20.pt'\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size: 86\n"
     ]
    }
   ],
   "source": [
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "PAD_IDX = 1\n",
    "CLS_IDX = 2\n",
    "SEP_IDX = 3\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer.create_using_stoi(simple_tokenizer, special_symbols, tokenizer_file)\n",
    "print('vocabulary_size:', tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar datos y crear dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4586399, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(raw_dataset)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts, is_consistents, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final, is_consistents_final = [], [], [], [], []\n",
    "\n",
    "    for sentence1, sentence2, is_next, is_consistent in zip(sentences1, sentences2, is_nexts, is_consistents):\n",
    "        # Tokenize each sentence\n",
    "        tokens1 = tokenizer.encode(sentence1).ids\n",
    "        tokens2 = tokenizer.encode(sentence2).ids\n",
    "        bert_input = ([CLS_IDX] + tokens1 + [SEP_IDX], tokens2 + [SEP_IDX])\n",
    "\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        bert_inputs_final.append(flatten(bert_input_padded))\n",
    "        segment_labels_final.append(flatten(segment_label_padded))\n",
    "        is_nexts_final.append(is_next)\n",
    "        is_consistents_final.append(is_consistent)\n",
    "\n",
    "    return bert_inputs_final, segment_labels_final, is_nexts_final, is_consistents_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df.iloc[:, 0]\n",
    "sentences2 = df.iloc[:, 1]\n",
    "is_nexts = df.iloc[:, 2]\n",
    "is_consistent = df.iloc[:, 3]\n",
    "bert_inputs_final, segment_labels_final, is_nexts_final, is_consistent_final = direct_prepare_bert_final_inputs(sentences1, sentences2, is_nexts, is_consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_final.shape=(4586399, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERT Input</th>\n",
       "      <th>Segment Label</th>\n",
       "      <th>Is Next</th>\n",
       "      <th>Is Consistent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 5, 6, 8, 3, 49, 6, 8, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 5, 6, 9, 3, 49, 6, 9, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 5, 6, 10, 3, 49, 6, 10, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 5, 6, 11, 3, 49, 6, 11, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 5, 6, 12, 3, 49, 6, 12, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[2, 5, 6, 13, 3, 49, 6, 13, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[2, 5, 6, 14, 3, 49, 6, 14, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[2, 5, 6, 15, 3, 49, 6, 15, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[2, 5, 6, 16, 3, 49, 6, 16, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[2, 5, 6, 17, 3, 49, 6, 17, 3, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2, 2, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          BERT Input                   Segment Label  Is Next  \\\n",
       "0    [2, 5, 6, 8, 3, 49, 6, 8, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "1    [2, 5, 6, 9, 3, 49, 6, 9, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "2  [2, 5, 6, 10, 3, 49, 6, 10, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "3  [2, 5, 6, 11, 3, 49, 6, 11, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "4  [2, 5, 6, 12, 3, 49, 6, 12, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "5  [2, 5, 6, 13, 3, 49, 6, 13, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "6  [2, 5, 6, 14, 3, 49, 6, 14, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "7  [2, 5, 6, 15, 3, 49, 6, 15, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "8  [2, 5, 6, 16, 3, 49, 6, 16, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "9  [2, 5, 6, 17, 3, 49, 6, 17, 3, 1]  [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]        1   \n",
       "\n",
       "   Is Consistent  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'BERT Input': bert_inputs_final,\n",
    "    'Segment Label': segment_labels_final,\n",
    "    'Is Next': is_nexts_final,\n",
    "    'Is Consistent': is_consistent_final\n",
    "})\n",
    "print(f'{df_final.shape=}')\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BERTDatasetNoLabels(df_final)\n",
    "bert_inputs, segment_labels, is_nexts, is_consistents = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch, segment_labels_batch, is_nexts_batch, is_consistents_batch = [], [], [], []\n",
    "\n",
    "    for bert_input, segment_label, is_next, is_consistent in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(torch.tensor(is_next, dtype=torch.long))\n",
    "        is_consistents_batch.append(torch.tensor(is_consistent, dtype=torch.long))\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch)\n",
    "    is_consistents_batch = torch.tensor(is_consistents_batch)\n",
    "\n",
    "    return bert_inputs_final.to(device), segment_labels_final.to(device), is_nexts_batch.to(device), is_consistents_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts, is_consistents = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert_embedding): BERTEmbedding(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(86, 16)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (segment_embedding): Embedding(3, 16)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (nextsentenceprediction): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (masked_language): Linear(in_features=16, out_features=86, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = tokenizer.get_vocab_size()  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 4  # Number of Transformer layers\n",
    "initial_heads = 4\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_model, weights_only=True,map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, segment_labels, is_nexts, is_consistents = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 49, 33, 60, 22, 47, 14, 3, 49, 33, 22, 47, 14, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 5, 37, 50, 25, 46, 17, 3, 49, 37, 50, 70, 25, 47, 70, 17, 3, 1, 1],\n",
       " [2, 70, 49, 38, 51, 16, 47, 13, 3, 5, 38, 51, 70, 16, 46, 70, 13, 3, 1, 1],\n",
       " [2, 5, 30, 57, 14, 46, 8, 3, 1, 49, 30, 57, 70, 14, 47, 70, 8, 3, 1, 1],\n",
       " [2, 5, 30, 66, 7, 47, 20, 3, 5, 30, 7, 47, 20, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 5, 29, 58, 25, 46, 16, 3, 49, 29, 58, 70, 25, 47, 70, 16, 3, 1, 1],\n",
       " [2, 49, 42, 62, 17, 46, 26, 3, 1, 5, 42, 62, 70, 17, 47, 70, 26, 3, 1, 1],\n",
       " [2, 49, 34, 59, 13, 47, 10, 3, 1, 5, 34, 59, 70, 13, 46, 70, 10, 3, 1, 1],\n",
       " [2, 48, 45, 15, 46, 21, 3, 1, 49, 45, 70, 15, 47, 70, 21, 3, 1, 1, 1, 1],\n",
       " [2, 5, 32, 59, 22, 46, 25, 3, 1, 1, 70, 49, 32, 59, 70, 22, 47, 70, 25, 3],\n",
       " [2, 70, 49, 42, 63, 14, 47, 16, 3, 5, 42, 63, 70, 14, 46, 70, 16, 3, 1, 1],\n",
       " [2, 49, 33, 66, 21, 47, 11, 3, 1, 5, 33, 66, 70, 21, 46, 70, 11, 3, 1, 1],\n",
       " [2, 49, 6, 55, 17, 47, 7, 3, 1, 5, 6, 55, 70, 17, 46, 70, 7, 3, 1, 1],\n",
       " [2, 70, 49, 42, 55, 12, 47, 13, 3, 48, 42, 55, 12, 47, 13, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 38, 62, 11, 47, 19, 3, 5, 38, 62, 11, 47, 19, 3, 1, 1, 1, 1],\n",
       " [2, 70, 49, 36, 64, 25, 46, 9, 3, 48, 36, 64, 25, 46, 9, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 42, 51, 16, 47, 25, 3, 49, 42, 51, 70, 16, 46, 70, 25, 3, 1, 1],\n",
       " [2, 5, 41, 65, 22, 47, 18, 3, 1, 49, 41, 65, 70, 22, 46, 70, 18, 3, 1, 1],\n",
       " [2, 70, 49, 43, 62, 15, 3, 5, 43, 62, 70, 15, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 5, 30, 53, 13, 47, 7, 3, 5, 30, 53, 13, 47, 7, 3, 1, 1, 1, 1],\n",
       " [2, 5, 36, 54, 70, 16, 46, 70, 20, 3, 48, 36, 54, 16, 47, 20, 3, 1, 1, 1],\n",
       " [2, 70, 5, 36, 54, 12, 47, 18, 3, 5, 36, 54, 12, 47, 18, 3, 1, 1, 1, 1],\n",
       " [2, 5, 34, 52, 12, 46, 16, 3, 1, 1, 70, 49, 34, 52, 70, 12, 47, 70, 16, 3],\n",
       " [2, 5, 41, 7, 47, 11, 3, 5, 41, 69, 7, 47, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 42, 62, 19, 47, 8, 3, 1, 1, 70, 49, 42, 62, 70, 19, 46, 70, 8, 3],\n",
       " [2, 49, 36, 26, 46, 16, 3, 49, 36, 60, 26, 46, 16, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 36, 52, 17, 47, 16, 3, 1, 5, 36, 52, 70, 17, 46, 70, 16, 3, 1, 1],\n",
       " [2, 70, 49, 6, 63, 14, 46, 24, 3, 48, 6, 63, 14, 46, 24, 3, 1, 1, 1, 1],\n",
       " [2, 5, 34, 51, 21, 46, 19, 3, 1, 1, 70, 49, 34, 51, 70, 21, 47, 70, 19, 3],\n",
       " [2, 70, 49, 36, 55, 10, 46, 17, 3, 48, 36, 55, 10, 46, 17, 3, 1, 1, 1, 1],\n",
       " [2, 5, 41, 57, 70, 17, 47, 70, 15, 3, 48, 41, 57, 17, 46, 15, 3, 1, 1, 1],\n",
       " [2, 5, 41, 62, 22, 47, 26, 3, 1, 1, 70, 49, 41, 62, 70, 22, 46, 70, 26, 3],\n",
       " [2, 5, 37, 54, 8, 47, 19, 3, 1, 49, 37, 54, 70, 8, 46, 70, 19, 3, 1, 1],\n",
       " [2, 49, 28, 59, 16, 46, 22, 3, 1, 5, 28, 59, 70, 16, 47, 70, 22, 3, 1, 1],\n",
       " [2, 70, 5, 33, 66, 15, 47, 21, 3, 5, 33, 66, 15, 47, 21, 3, 1, 1, 1, 1],\n",
       " [2, 70, 49, 41, 50, 9, 46, 10, 3, 49, 41, 50, 9, 46, 10, 3, 1, 1, 1, 1],\n",
       " [2, 49, 44, 63, 12, 46, 25, 3, 49, 44, 12, 46, 25, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 36, 69, 7, 47, 11, 3, 1, 5, 36, 69, 70, 7, 46, 70, 11, 3, 1, 1],\n",
       " [2, 5, 43, 65, 70, 14, 46, 70, 15, 3, 48, 43, 65, 14, 47, 15, 3, 1, 1, 1],\n",
       " [2, 5, 45, 50, 16, 46, 18, 3, 1, 1, 70, 49, 45, 50, 70, 16, 47, 70, 18, 3],\n",
       " [2, 49, 6, 61, 7, 47, 13, 3, 49, 6, 7, 47, 13, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 28, 61, 70, 25, 47, 70, 15, 3, 48, 28, 61, 25, 46, 15, 3, 1, 1, 1],\n",
       " [2, 5, 35, 64, 8, 46, 7, 3, 1, 49, 35, 64, 70, 8, 47, 70, 7, 3, 1, 1],\n",
       " [2, 5, 6, 14, 46, 15, 3, 5, 6, 51, 14, 46, 15, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 45, 68, 9, 46, 8, 3, 49, 45, 68, 9, 46, 8, 3, 1, 1, 1, 1],\n",
       " [2, 49, 28, 57, 20, 3, 5, 28, 57, 70, 20, 3, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 42, 64, 23, 46, 15, 3, 1, 5, 42, 64, 70, 23, 47, 70, 15, 3, 1, 1],\n",
       " [2, 5, 39, 69, 25, 47, 7, 3, 1, 49, 39, 69, 70, 25, 46, 70, 7, 3, 1, 1],\n",
       " [2, 5, 37, 65, 7, 46, 24, 3, 1, 1, 70, 49, 37, 65, 70, 7, 47, 70, 24, 3],\n",
       " [2, 49, 28, 12, 46, 23, 3, 49, 28, 68, 12, 46, 23, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 38, 66, 8, 47, 22, 3, 5, 38, 8, 47, 22, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 40, 64, 9, 47, 15, 3, 1, 5, 40, 64, 70, 9, 46, 70, 15, 3, 1, 1],\n",
       " [2, 70, 49, 36, 22, 47, 12, 3, 5, 36, 70, 22, 46, 70, 12, 3, 1, 1, 1, 1],\n",
       " [2, 5, 42, 54, 70, 14, 46, 70, 19, 3, 48, 42, 54, 14, 47, 19, 3, 1, 1, 1],\n",
       " [2, 5, 41, 54, 70, 7, 46, 70, 16, 3, 48, 41, 54, 7, 47, 16, 3, 1, 1, 1],\n",
       " [2, 5, 38, 67, 21, 47, 22, 3, 5, 38, 21, 47, 22, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 28, 60, 70, 15, 46, 70, 13, 3, 48, 28, 60, 15, 47, 13, 3, 1, 1, 1],\n",
       " [2, 49, 43, 61, 24, 46, 11, 3, 49, 43, 24, 46, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 39, 60, 20, 46, 12, 3, 1, 1, 70, 49, 39, 60, 70, 20, 47, 70, 12, 3],\n",
       " [2, 5, 39, 57, 70, 12, 47, 70, 7, 3, 48, 39, 57, 12, 46, 7, 3, 1, 1, 1],\n",
       " [2, 70, 5, 45, 61, 19, 47, 14, 3, 49, 45, 61, 70, 19, 46, 70, 14, 3, 1, 1],\n",
       " [2, 70, 5, 27, 54, 7, 47, 17, 3, 49, 27, 54, 70, 7, 46, 70, 17, 3, 1, 1],\n",
       " [2, 70, 5, 33, 53, 16, 47, 9, 3, 49, 33, 53, 70, 16, 46, 70, 9, 3, 1, 1],\n",
       " [2, 5, 27, 21, 47, 24, 3, 5, 27, 69, 21, 47, 24, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 30, 54, 15, 47, 14, 3, 49, 30, 54, 15, 47, 14, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 28, 65, 19, 46, 26, 3, 49, 28, 65, 70, 19, 47, 70, 26, 3, 1, 1],\n",
       " [2, 49, 39, 22, 47, 19, 3, 49, 39, 69, 22, 47, 19, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 28, 58, 70, 10, 47, 70, 13, 3, 48, 28, 58, 10, 46, 13, 3, 1, 1, 1],\n",
       " [2, 5, 39, 53, 70, 26, 46, 70, 8, 3, 48, 39, 53, 26, 47, 8, 3, 1, 1, 1],\n",
       " [2, 49, 35, 69, 21, 47, 26, 3, 49, 35, 21, 47, 26, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 34, 52, 10, 46, 24, 3, 1, 5, 34, 52, 70, 10, 47, 70, 24, 3, 1, 1],\n",
       " [2, 49, 34, 14, 46, 7, 3, 49, 34, 59, 14, 46, 7, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 40, 50, 12, 46, 23, 3, 48, 40, 50, 12, 46, 23, 3, 1, 1, 1, 1],\n",
       " [2, 49, 44, 55, 8, 46, 11, 3, 49, 44, 8, 46, 11, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 33, 57, 22, 47, 18, 3, 48, 33, 57, 22, 47, 18, 3, 1, 1, 1, 1],\n",
       " [2, 49, 40, 19, 47, 24, 3, 49, 40, 53, 19, 47, 24, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 34, 55, 14, 47, 10, 3, 49, 34, 55, 14, 47, 10, 3, 1, 1, 1, 1],\n",
       " [2, 5, 34, 24, 46, 8, 3, 5, 34, 62, 24, 46, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 35, 54, 16, 46, 12, 3, 49, 35, 16, 46, 12, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 5, 31, 65, 19, 46, 11, 3, 5, 31, 65, 19, 46, 11, 3, 1, 1, 1, 1],\n",
       " [2, 5, 44, 67, 26, 47, 16, 3, 5, 44, 26, 47, 16, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 38, 67, 12, 47, 25, 3, 1, 49, 38, 67, 70, 12, 46, 70, 25, 3, 1, 1],\n",
       " [2, 70, 49, 39, 61, 8, 46, 21, 3, 5, 39, 61, 70, 8, 47, 70, 21, 3, 1, 1],\n",
       " [2, 49, 43, 53, 25, 47, 17, 3, 1, 5, 43, 53, 70, 25, 46, 70, 17, 3, 1, 1],\n",
       " [2, 5, 30, 26, 46, 8, 3, 5, 30, 61, 26, 46, 8, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 33, 61, 70, 7, 46, 70, 21, 3, 48, 33, 61, 7, 47, 21, 3, 1, 1, 1],\n",
       " [2, 49, 6, 65, 13, 46, 20, 3, 1, 5, 6, 65, 70, 13, 47, 70, 20, 3, 1, 1],\n",
       " [2, 5, 27, 60, 24, 47, 18, 3, 1, 1, 70, 49, 27, 60, 70, 24, 46, 70, 18, 3],\n",
       " [2, 70, 49, 39, 69, 26, 47, 25, 3, 5, 39, 69, 70, 26, 46, 70, 25, 3, 1, 1],\n",
       " [2, 49, 6, 66, 9, 47, 13, 3, 49, 6, 9, 47, 13, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 41, 54, 7, 47, 16, 3, 5, 41, 54, 70, 7, 46, 70, 16, 3, 1, 1],\n",
       " [2, 49, 40, 18, 46, 14, 3, 49, 40, 66, 18, 46, 14, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 36, 62, 70, 9, 46, 70, 11, 3, 48, 36, 62, 9, 47, 11, 3, 1, 1, 1],\n",
       " [2, 5, 6, 57, 70, 12, 46, 70, 11, 3, 48, 6, 57, 12, 47, 11, 3, 1, 1, 1],\n",
       " [2, 70, 5, 43, 53, 16, 46, 7, 3, 5, 43, 53, 16, 46, 7, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 43, 60, 8, 47, 21, 3, 5, 43, 60, 8, 47, 21, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 6, 55, 19, 46, 7, 3, 49, 6, 55, 70, 19, 47, 70, 7, 3, 1, 1],\n",
       " [2, 70, 49, 32, 62, 20, 47, 16, 3, 49, 32, 62, 20, 47, 16, 3, 1, 1, 1, 1],\n",
       " [2, 5, 35, 23, 47, 15, 3, 5, 35, 55, 23, 47, 15, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 32, 17, 47, 10, 3, 49, 32, 17, 47, 10, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 38, 70, 20, 46, 70, 26, 3, 5, 38, 20, 47, 26, 3, 1, 1, 1, 1],\n",
       " [2, 70, 49, 32, 63, 11, 46, 26, 3, 48, 32, 63, 11, 46, 26, 3, 1, 1, 1, 1],\n",
       " [2, 70, 5, 6, 54, 7, 46, 23, 3, 49, 6, 54, 70, 7, 47, 70, 23, 3, 1, 1],\n",
       " [2, 5, 30, 53, 22, 46, 18, 3, 5, 30, 22, 46, 18, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 70, 49, 39, 68, 7, 46, 19, 3, 49, 39, 68, 7, 46, 19, 3, 1, 1, 1, 1],\n",
       " [2, 70, 49, 30, 59, 16, 47, 9, 3, 49, 30, 59, 16, 47, 9, 3, 1, 1, 1, 1],\n",
       " [2, 49, 42, 11, 46, 19, 3, 49, 42, 61, 11, 46, 19, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 34, 59, 16, 46, 25, 3, 1, 1, 70, 49, 34, 59, 70, 16, 47, 70, 25, 3],\n",
       " [2, 70, 49, 40, 65, 23, 46, 25, 3, 49, 40, 65, 23, 46, 25, 3, 1, 1, 1, 1],\n",
       " [2, 70, 49, 42, 57, 11, 3, 5, 42, 57, 70, 11, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 35, 57, 23, 47, 11, 3, 1, 49, 35, 57, 70, 23, 46, 70, 11, 3, 1, 1],\n",
       " [2, 70, 49, 29, 66, 21, 47, 20, 3, 49, 29, 66, 21, 47, 20, 3, 1, 1, 1, 1],\n",
       " [2, 5, 33, 51, 21, 46, 14, 3, 5, 33, 21, 46, 14, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 38, 8, 46, 11, 3, 49, 38, 8, 46, 11, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 40, 57, 26, 47, 14, 3, 49, 40, 26, 47, 14, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 36, 61, 70, 8, 47, 70, 23, 3, 48, 36, 61, 8, 46, 23, 3, 1, 1, 1],\n",
       " [2, 49, 39, 64, 19, 47, 18, 3, 1, 5, 39, 64, 70, 19, 46, 70, 18, 3, 1, 1],\n",
       " [2, 49, 27, 56, 14, 46, 18, 3, 1, 5, 27, 56, 70, 14, 47, 70, 18, 3, 1, 1],\n",
       " [2, 5, 28, 57, 70, 25, 46, 70, 13, 3, 48, 28, 57, 25, 47, 13, 3, 1, 1, 1],\n",
       " [2, 49, 45, 63, 21, 46, 12, 3, 49, 45, 21, 46, 12, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 39, 70, 16, 46, 70, 18, 3, 48, 39, 16, 47, 18, 3, 1, 1, 1, 1, 1],\n",
       " [2, 5, 37, 62, 70, 20, 47, 70, 17, 3, 48, 37, 62, 20, 46, 17, 3, 1, 1, 1],\n",
       " [2, 49, 39, 56, 18, 46, 16, 3, 1, 5, 39, 56, 70, 18, 47, 70, 16, 3, 1, 1],\n",
       " [2, 70, 49, 31, 50, 18, 46, 25, 3, 5, 31, 50, 70, 18, 47, 70, 25, 3, 1, 1],\n",
       " [2, 5, 29, 69, 24, 46, 13, 3, 1, 1, 70, 49, 29, 69, 70, 24, 47, 70, 13, 3],\n",
       " [2, 49, 45, 18, 47, 10, 3, 49, 45, 58, 18, 47, 10, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 49, 44, 56, 25, 47, 24, 3, 49, 44, 25, 47, 24, 3, 1, 1, 1, 1, 1, 1],\n",
       " [2, 5, 45, 12, 46, 11, 3, 5, 45, 52, 12, 46, 11, 3, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.cpu().numpy().transpose(1, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    next_sentence_prediction, masked_language = model(bert_inputs.to(device), segment_labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_sentences(bert_input):\n",
    "    def zero_pad_list_pair(pair_, pad=PAD_IDX):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0] + pair[1]\n",
    "\n",
    "    tokens = bert_input.cpu().numpy().tolist()\n",
    "    assert(len(tokens) % 2 == 0), len(tokens)\n",
    "    try:\n",
    "        len_sents = tokens.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(tokens)\n",
    "        print(tokenizer.decode(tokens))\n",
    "        raise Exception(e)\n",
    "    first = tokens[1:len_sents - 1]\n",
    "    second_ = tokens[len_sents:]\n",
    "    try:\n",
    "        sep_in_second = second_.index(SEP_IDX)\n",
    "    except Exception as e:\n",
    "        print(second_)\n",
    "        print(tokenizer.decode(second_))\n",
    "        raise Exception(e)\n",
    "    second = second_[:sep_in_second]\n",
    "    bert_input = ([CLS_IDX] + second + [SEP_IDX], first + [SEP_IDX])\n",
    "    # Create segment labels for each pair of sentences\n",
    "    segment_label = ([1] * len(bert_input[0]), [2] * len(bert_input[1]))\n",
    "    # Zero-pad the bert_input and bert_label and segment_label\n",
    "    bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "    bert_input_padded = torch.tensor(bert_input_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "    segment_label_padded = torch.tensor(segment_label_padded, dtype=torch.long).unsqueeze(dim=1)\n",
    "    return bert_input_padded, segment_label_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(input, segment_label, is_next, device):\n",
    "    with torch.no_grad():\n",
    "        model_prediction, _ = model(input.to(device), segment_label.to(device))\n",
    "    logits = torch.softmax(model_prediction, dim=1)\n",
    "    prediction = torch.argmax(logits, dim=1)     \n",
    "    correct = prediction == is_next\n",
    "    return correct, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7652b1c5e3214867960d3fe4b7109814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.0\n",
      "Accuracy: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "test = list()\n",
    "golden = list()\n",
    "for bert_inputs, segment_labels, is_nexts, is_consistents in tqdm(dataloader):\n",
    "    corrects,_ = get_prediction(\n",
    "        input=bert_inputs,\n",
    "        segment_label=segment_labels,\n",
    "        is_next=is_nexts,\n",
    "        device=device\n",
    "    )\n",
    "    for idx, correct in enumerate(corrects):\n",
    "        if correct:\n",
    "            bert_input = bert_inputs.transpose(1,0)[idx]\n",
    "            bert_input_reversed, segment_label = invert_sentences(bert_input)\n",
    "            check, prediction = get_prediction(\n",
    "                input=bert_input_reversed, \n",
    "                segment_label=segment_label,\n",
    "                is_next=is_consistents[idx],\n",
    "                device=device\n",
    "            )\n",
    "            test.append(prediction.cpu().item())\n",
    "            golden.append(is_consistents[idx].cpu())\n",
    "acc = accuracy_score(golden, test)\n",
    "f1 = f1_score(golden, test)    \n",
    "\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
