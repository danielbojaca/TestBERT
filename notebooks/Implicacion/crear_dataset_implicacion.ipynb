{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del dataset para oraciones de relaciones lógicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.config import PATHS\n",
    "from src.utils.utils_vocab import BasicTokenizer\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = PATHS[\"training_data_folder\"]\n",
    "tokenizer_folder = PATHS[\"tokenizer_folder\"]\n",
    "\n",
    "raw_dataset = dataset_folder/'implicacion_5.csv'\n",
    "# raw_dataset = dataset_folder/'implicacion_10.csv'\n",
    "# raw_dataset = dataset_folder/'implicacion_15.csv'\n",
    "# raw_dataset = dataset_folder/'implicacion_20.csv'\n",
    "\n",
    "tokenizer_file = tokenizer_folder/'tokenizer_5.pkl'\n",
    "# tokenizer_file = tokenizer_folder/'tokenizer_10.pkl'\n",
    "# tokenizer_file = tokenizer_folder/'tokenizer_15.pkl'\n",
    "# tokenizer_file = tokenizer_folder/'tokenizer_20.pkl'\n",
    "\n",
    "csv_file_path = dataset_folder/'bert_data_implicacion_5.csv'\n",
    "# csv_file_path = dataset_folder/'bert_data_implicacion_10.csv'\n",
    "# csv_file_path = dataset_folder/'bert_data_implicacion_15.csv'\n",
    "# csv_file_path = dataset_folder/'bert_data_implicacion_20.csv'\n",
    "\n",
    "jabberwockie_dataset = dataset_folder/'implicacion_jabberwockie_5.csv'\n",
    "# csv_file_path =dataset_folder/'bert_data_implicacion_jabberwockie_5.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence 1</th>\n",
       "      <th>Sentence 2</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Consistencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo abuelo acuerda</td>\n",
       "      <td>algún abuelo acuerda</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>todo abuelo aguanta</td>\n",
       "      <td>algún abuelo aguanta</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo abuelo ama</td>\n",
       "      <td>algún abuelo ama</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>todo abuelo amanece</td>\n",
       "      <td>algún abuelo amanece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todo abuelo anochece</td>\n",
       "      <td>algún abuelo anochece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence 1              Sentence 2  Relation  Consistencia\n",
       "0   todo abuelo acuerda     algún abuelo acuerda         1             0\n",
       "1   todo abuelo aguanta     algún abuelo aguanta         1             0\n",
       "2       todo abuelo ama         algún abuelo ama         1             0\n",
       "3   todo abuelo amanece     algún abuelo amanece         1             0\n",
       "4  todo abuelo anochece    algún abuelo anochece         1             0"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df1 = pd.read_csv(raw_dataset, names=['Sentence 1', 'Sentence 2', 'Relation', 'Consistencia']) \n",
    "words_jabberwockie = pd.read_csv(jabberwockie_dataset, names=['Sentence 1', 'Sentence 2', 'Relation', 'Consistencia'])\n",
    "words_df = pd.concat([words_df1, words_jabberwockie], ignore_index=True)\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo abuelo acuerda ', 'todo abuelo aguanta ', 'todo abuelo ama ', 'todo abuelo amanece ', 'todo abuelo anochece ', 'todo abuelo aparece ', 'todo abuelo aplaude ', 'todo abuelo avanza ', 'todo abuelo baja ', 'todo abuelo bosteza ']\n",
      "9210600\n"
     ]
    }
   ],
   "source": [
    "words = list(words_df.iloc[:,0].values)\n",
    "words += list(words_df.iloc[:,1].values)\n",
    "print(words[:10])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ama', 'amanece', 'carraspea', 'amarillo', 'contempla', 'no', 'árbol', 'asombroso', 'brillante', 'avión', 'débil', 'burro', 'dulce', 'alce', 'y', 'dormínico', 'aparece', 'albañil', 'apagado', 'brinca', 'canta', 'blicket', 'abuelo', 'avanza', 'bosteza', 'celebra', 'cable', 'arquitecto', 'brispado', 'barril', 'áspero', 'alto', 'bosque', 'claro', 'flakle', 'caballo', 'jufzyl', 'calla', 'camino', 'algún', 'flexivo', 'jufmoq', 'caracol', 'dernea', 'auto', 'delgado', 'agujero', 'claribundo', 'todo', 'camello', 'o', 'bajo', 'flajuf', 'aplaude', 'animal', 'caliente', 'amplio', 'complejo', 'drifla', 'cambia', 'actor', 'amargo', 'florido', 'blando', 'alegre', 'acuerda', 'barco', 'baja', 'brilca', 'ningún', 'cae', 'aguanta', 'azul', 'converge', 'carro', 'cansado', 'bliscea', 'anochece', 'brunza', 'difícil', 'camina']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "y = [w.strip().split(' ')  for w in words]\n",
    "y = [x for w in y for x in w]\n",
    "y = [w for w in y if w != '']\n",
    "y = list(set(y))\n",
    "print(y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols for the tokenizer\n",
    "special_symbols = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "# Create tokenizer from words\n",
    "\n",
    "simple_tokenizer = lambda tokens_string: tokens_string.strip().split()\n",
    "tokenizer = BasicTokenizer(simple_tokenizer, special_symbols)\n",
    "tokenizer.initialize_from_iterable(words)\n",
    "\n",
    "# Save to file\n",
    "tokenizer.save(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.itos)\n",
    "print(f'Tamaño del vocabulario: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]', 'todo', 'abuelo', 'acuerda', 'aguanta', 'ama', 'amanece', 'anochece', 'aparece', 'aplaude', 'avanza', 'baja', 'bosteza', 'brinca', 'cae', 'calla', 'cambia', 'camina', 'canta', 'carraspea', 'celebra', 'contempla', 'converge', 'actor', 'agujero', 'albañil', 'alce', 'animal', 'árbol', 'arquitecto', 'auto', 'avión', 'barco', 'barril', 'bosque', 'burro', 'caballo', 'cable', 'camello', 'camino', 'caracol', 'carro', 'y', 'o', 'ningún', 'algún', 'alegre', 'alto', 'amargo', 'amplio', 'amarillo', 'apagado', 'asombroso', 'áspero', 'azul', 'bajo', 'blando', 'brillante', 'caliente', 'cansado', 'claro', 'complejo', 'débil', 'delgado', 'difícil', 'dulce', 'no', 'blicket', 'bliscea', 'brilca', 'brunza', 'dernea', 'drifla', 'flajuf', 'flakle', 'jufmoq', 'jufzyl', 'brispado', 'claribundo', 'dormínico', 'florido', 'flexivo']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4586400, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "      <th>consistencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4586395</th>\n",
       "      <td>no algún carro converge y camina</td>\n",
       "      <td>algún carro converge y camina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586396</th>\n",
       "      <td>no algún carro converge y canta</td>\n",
       "      <td>algún carro converge y canta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586397</th>\n",
       "      <td>no algún carro converge y carraspea</td>\n",
       "      <td>algún carro converge y carraspea</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586398</th>\n",
       "      <td>no algún carro converge y celebra</td>\n",
       "      <td>algún carro converge y celebra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586399</th>\n",
       "      <td>no algún carro converge y contempla</td>\n",
       "      <td>algún carro converge y contempla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence 1  \\\n",
       "4586395     no algún carro converge y camina    \n",
       "4586396      no algún carro converge y canta    \n",
       "4586397  no algún carro converge y carraspea    \n",
       "4586398    no algún carro converge y celebra    \n",
       "4586399  no algún carro converge y contempla    \n",
       "\n",
       "                                sentence 2  relation  consistencia  \n",
       "4586395      algún carro converge y camina         0             0  \n",
       "4586396       algún carro converge y canta         0             0  \n",
       "4586397   algún carro converge y carraspea         0             0  \n",
       "4586398     algún carro converge y celebra         0             0  \n",
       "4586399   algún carro converge y contempla         0             0  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = pd.read_csv(raw_dataset, names=['sentence 1', 'sentence 2', 'relation', 'consistencia'])\n",
    "# sentences_df = pd.read_csv(jabberwockie_dataset, names=['sentence 1', 'sentence 2', 'relation', 'consistencia'])\n",
    "print(sentences_df.shape)\n",
    "sentences_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "      <th>consistencia</th>\n",
       "      <th>tokens sentence 1</th>\n",
       "      <th>tokens sentence 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo abuelo acuerda</td>\n",
       "      <td>algún abuelo acuerda</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[algún, abuelo, acuerda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>todo abuelo aguanta</td>\n",
       "      <td>algún abuelo aguanta</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[algún, abuelo, aguanta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo abuelo ama</td>\n",
       "      <td>algún abuelo ama</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, ama]</td>\n",
       "      <td>[algún, abuelo, ama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>todo abuelo amanece</td>\n",
       "      <td>algún abuelo amanece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, amanece]</td>\n",
       "      <td>[algún, abuelo, amanece]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todo abuelo anochece</td>\n",
       "      <td>algún abuelo anochece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[algún, abuelo, anochece]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sentence 1              sentence 2  relation  consistencia  \\\n",
       "0   todo abuelo acuerda     algún abuelo acuerda         1             0   \n",
       "1   todo abuelo aguanta     algún abuelo aguanta         1             0   \n",
       "2       todo abuelo ama         algún abuelo ama         1             0   \n",
       "3   todo abuelo amanece     algún abuelo amanece         1             0   \n",
       "4  todo abuelo anochece    algún abuelo anochece         1             0   \n",
       "\n",
       "          tokens sentence 1          tokens sentence 2  \n",
       "0   [todo, abuelo, acuerda]   [algún, abuelo, acuerda]  \n",
       "1   [todo, abuelo, aguanta]   [algún, abuelo, aguanta]  \n",
       "2       [todo, abuelo, ama]       [algún, abuelo, ama]  \n",
       "3   [todo, abuelo, amanece]   [algún, abuelo, amanece]  \n",
       "4  [todo, abuelo, anochece]  [algún, abuelo, anochece]  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['tokens sentence 1'] = sentences_df['sentence 1'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "sentences_df['tokens sentence 2'] = sentences_df['sentence 2'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_true_false(p):\n",
    "    # Create a Bernoulli distribution with probability p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
    "    # Sample from this distribution and convert 1 to True and 0 to False\n",
    "    return bernoulli_dist.sample().item() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    # Decide whether to mask this token (20% chance)\n",
    "    mask = bernoulli_true_false(0.2)\n",
    "\n",
    "    # If mask is False, immediately return with '[PAD]' label\n",
    "    if not mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # If mask is True, proceed with further operations\n",
    "    # Randomly decide on an operation (50% chance each)\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    # Case 1: If mask, random_opp, and random_swich are True\n",
    "    if mask and random_opp and random_swich:\n",
    "        # Replace the token with '[MASK]' and set label to a random token\n",
    "        mask_label = tokenizer.decode(torch.randint(0, VOCAB_SIZE, (1,)))[0]\n",
    "        token_ = '[MASK]'\n",
    "\n",
    "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
    "    elif mask and random_opp and not random_swich:\n",
    "        # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    # Case 3: If mask is True, but random_opp is False\n",
    "    else:\n",
    "        # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] abuelo \t Actual token *abuelo* is masked with '[MASK]'\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "[MASK] difícil \t Actual token *abuelo* is replaced with random token #difícil#\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n",
      "abuelo [PAD] \t Actual token *abuelo* is left unchanged\n"
     ]
    }
   ],
   "source": [
    "# Test Masking\n",
    "torch.manual_seed(100)\n",
    "for l in range(10):\n",
    "  token=\"abuelo\"\n",
    "  token_,label=Masking(token)\n",
    "  if token==token_ and label==\"[PAD]\":\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is left unchanged\")\n",
    "  elif token_==\"[MASK]\" and label==token:\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is masked with '{token_}'\")\n",
    "  else:\n",
    "    print(token_,label,f\"\\t Actual token *{token}* is replaced with random token #{label}#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
    "    \"\"\"\n",
    "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
    "\n",
    "    \"\"\"\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    raw_tokens = []  # List to store raw tokens if needed\n",
    "\n",
    "    for token in tokens:\n",
    "        # Apply BERT's MLM masking strategy to the token\n",
    "        masked_token, mask_label = Masking(token)\n",
    "\n",
    "        # Append the processed token and its label to the current sentence and label list\n",
    "        bert_input.append(masked_token)\n",
    "        bert_label.append(mask_label)\n",
    "\n",
    "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens.append(token)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without raw tokens: \t  \n",
      " \t original_input is: \t  algún abuelo alegre no ama \n",
      " \t bert_input is: \t  ['[MASK]', 'abuelo', 'alegre', 'no', 'ama'] \n",
      " \t bert_label is: \t  ['algún', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "With raw tokens: \t  \n",
      " \t original_input is: \t  algún abuelo alegre no ama \n",
      " \t bert_input is: \t  ['algún', 'abuelo', 'alegre', 'no', 'ama'] \n",
      " \t bert_label is: \t  ['[PAD]', '[PAD]', '[PAD]', 'no', '[PAD]'] \n",
      " \t raw_tokens_list is: \t  ['algún', 'abuelo', 'alegre', 'no', 'ama']\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "# original_input=\"The sun sets behind the distant mountains.\"\n",
    "original_input = \"algún abuelo alegre no ama\"\n",
    "tokens=tokenizer.encode(original_input).tokens\n",
    "bert_input, bert_label= prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "print(\"Without raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label)\n",
    "print(\"-\"*200)\n",
    "torch.manual_seed(200)\n",
    "bert_input, bert_label, raw_tokens_list = prepare_for_mlm(tokens, include_raw_tokens=True)\n",
    "print(\"With raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label,\"\\n \\t raw_tokens_list is: \\t \", raw_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence 1</th>\n",
       "      <th>sentence 2</th>\n",
       "      <th>relation</th>\n",
       "      <th>consistencia</th>\n",
       "      <th>tokens sentence 1</th>\n",
       "      <th>tokens sentence 2</th>\n",
       "      <th>bert_input 1</th>\n",
       "      <th>bert_label 1</th>\n",
       "      <th>bert_input 2</th>\n",
       "      <th>bert_label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo abuelo acuerda</td>\n",
       "      <td>algún abuelo acuerda</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[algún, abuelo, acuerda]</td>\n",
       "      <td>[todo, abuelo, acuerda]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], acuerda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>todo abuelo aguanta</td>\n",
       "      <td>algún abuelo aguanta</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[algún, abuelo, aguanta]</td>\n",
       "      <td>[todo, abuelo, aguanta]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, aguanta]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>todo abuelo ama</td>\n",
       "      <td>algún abuelo ama</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, ama]</td>\n",
       "      <td>[algún, abuelo, ama]</td>\n",
       "      <td>[todo, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], ama]</td>\n",
       "      <td>[algún, [MASK], ama]</td>\n",
       "      <td>[[PAD], abuelo, [PAD]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>todo abuelo amanece</td>\n",
       "      <td>algún abuelo amanece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, amanece]</td>\n",
       "      <td>[algún, abuelo, amanece]</td>\n",
       "      <td>[todo, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], amanece]</td>\n",
       "      <td>[algún, abuelo, [MASK]]</td>\n",
       "      <td>[[PAD], [PAD], amanece]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todo abuelo anochece</td>\n",
       "      <td>algún abuelo anochece</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[algún, abuelo, anochece]</td>\n",
       "      <td>[todo, abuelo, anochece]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "      <td>[algún, abuelo, anochece]</td>\n",
       "      <td>[[PAD], [PAD], [PAD]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sentence 1              sentence 2  relation  consistencia  \\\n",
       "0   todo abuelo acuerda     algún abuelo acuerda         1             0   \n",
       "1   todo abuelo aguanta     algún abuelo aguanta         1             0   \n",
       "2       todo abuelo ama         algún abuelo ama         1             0   \n",
       "3   todo abuelo amanece     algún abuelo amanece         1             0   \n",
       "4  todo abuelo anochece    algún abuelo anochece         1             0   \n",
       "\n",
       "          tokens sentence 1          tokens sentence 2  \\\n",
       "0   [todo, abuelo, acuerda]   [algún, abuelo, acuerda]   \n",
       "1   [todo, abuelo, aguanta]   [algún, abuelo, aguanta]   \n",
       "2       [todo, abuelo, ama]       [algún, abuelo, ama]   \n",
       "3   [todo, abuelo, amanece]   [algún, abuelo, amanece]   \n",
       "4  [todo, abuelo, anochece]  [algún, abuelo, anochece]   \n",
       "\n",
       "               bert_input 1             bert_label 1  \\\n",
       "0   [todo, abuelo, acuerda]    [[PAD], [PAD], [PAD]]   \n",
       "1   [todo, abuelo, aguanta]    [[PAD], [PAD], [PAD]]   \n",
       "2    [todo, abuelo, [MASK]]      [[PAD], [PAD], ama]   \n",
       "3    [todo, abuelo, [MASK]]  [[PAD], [PAD], amanece]   \n",
       "4  [todo, abuelo, anochece]    [[PAD], [PAD], [PAD]]   \n",
       "\n",
       "                bert_input 2             bert_label 2  \n",
       "0    [algún, abuelo, [MASK]]  [[PAD], [PAD], acuerda]  \n",
       "1   [algún, abuelo, aguanta]    [[PAD], [PAD], [PAD]]  \n",
       "2       [algún, [MASK], ama]   [[PAD], abuelo, [PAD]]  \n",
       "3    [algún, abuelo, [MASK]]  [[PAD], [PAD], amanece]  \n",
       "4  [algún, abuelo, anochece]    [[PAD], [PAD], [PAD]]  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['bert_input_label 1'] = sentences_df['tokens sentence 1'].apply(lambda x: prepare_for_mlm(x))\n",
    "sentences_df['bert_input 1'] = sentences_df['bert_input_label 1'].apply(lambda x: x[0])\n",
    "sentences_df['bert_label 1'] = sentences_df['bert_input_label 1'].apply(lambda x: x[1])\n",
    "sentences_df['bert_input_label 2'] = sentences_df['tokens sentence 2'].apply(lambda x: prepare_for_mlm(x))\n",
    "sentences_df['bert_input 2'] = sentences_df['bert_input_label 2'].apply(lambda x: x[0])\n",
    "sentences_df['bert_label 2'] = sentences_df['bert_input_label 2'].apply(lambda x: x[1])\n",
    "del sentences_df['bert_input_label 1']\n",
    "del sentences_df['bert_input_label 2']\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_pair = sentences_df[['bert_input 1', 'bert_input 2']].values.tolist()\n",
    "input_masked_labels_pair = sentences_df[['bert_label 1', 'bert_label 2']].values.tolist()\n",
    "relations = sentences_df['relation'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_nsp(input_sentences_pair, input_masked_labels_pair, relations):\n",
    "    \"\"\"\n",
    "    Prepares data for understanding logical relationship.\n",
    "\n",
    "    Args:\n",
    "    input_sentences (list): List of tokenized sentences.\n",
    "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
    "\n",
    "    Returns:\n",
    "    bert_input (list): List of sentence pairs for BERT input.\n",
    "    bert_label (list): List of masked labels for the sentence pairs.\n",
    "    is_next (list): Binary label list where 1 indicates 'logical relationship' and 0 indicates 'not logical relationship'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "    if len(input_sentences_pair) != len(input_masked_labels_pair):\n",
    "        raise ValueError(\"Both lists, input_sentences_pair and input_masked_labels_pair, must have the same number of items.\")\n",
    "    if len(input_sentences_pair) != len(relations):\n",
    "        raise ValueError(\"Both lists, input_sentences_pair and relations, must have the same number of items.\")\n",
    "\n",
    "    bert_input = []\n",
    "    bert_label = []\n",
    "    is_next = []\n",
    "\n",
    "    for sentence_pair, masked_pair, relation in zip(input_sentences_pair, input_masked_labels_pair, relations):\n",
    "        # append list and add  '[CLS]' and  '[SEP]' tokens\n",
    "        bert_input.append([['[CLS]'] + sentence_pair[0] + ['[SEP]'], sentence_pair[1] + ['[SEP]']])\n",
    "        bert_label.append([['[PAD]'] + masked_pair[0] + ['[PAD]'], masked_pair[1]+ ['[PAD]']])\n",
    "        is_next.append(relation)  # Label 1 indicates these sentences have the required logical relationship\n",
    "\n",
    "    return bert_input, bert_label, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, bert_labels, is_nexts = process_for_nsp(input_sentences_pair, input_masked_labels_pair, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[CLS]', 'todo', 'abuelo', 'acuerda', '[SEP]'],\n",
       "  ['algún', 'abuelo', '[MASK]', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'aguanta', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'aguanta', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', '[MASK]', '[SEP]'],\n",
       "  ['algún', '[MASK]', 'ama', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', '[MASK]', '[SEP]'],\n",
       "  ['algún', 'abuelo', '[MASK]', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'anochece', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'anochece', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'aparece', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'aparece', '[SEP]']],\n",
       " [['[CLS]', 'todo', '[MASK]', 'aplaude', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'aplaude', '[SEP]']],\n",
       " [['[CLS]', '[MASK]', 'abuelo', 'avanza', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'avanza', '[SEP]']],\n",
       " [['[CLS]', 'todo', 'abuelo', 'baja', '[SEP]'],\n",
       "  ['[MASK]', 'abuelo', 'baja', '[SEP]']],\n",
       " [['[CLS]', 'todo', '[MASK]', '[MASK]', '[SEP]'],\n",
       "  ['algún', 'abuelo', 'bosteza', '[SEP]']]]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts, to_tensor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair = deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #transform tokens to vocab indices\n",
    "    tokens_to_index=lambda tokens: [tokenizer.stoi[token] for token in tokens]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        #convert to tensors\n",
    "        if to_tensor:\n",
    "\n",
    "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "            # bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
    "            # bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            # segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
    "            bert_inputs_final.append(tokens_to_index(flatten(bert_input_padded)))\n",
    "            bert_labels_final.append(tokens_to_index(flatten(bert_label_padded)))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "        else:\n",
    "          # Flatten the padded inputs and labels\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            bert_labels_final.append(flatten(bert_label_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 6, 7, 3, 49, 6, 4, 3, 1],\n",
       " [2, 5, 6, 8, 3, 49, 6, 8, 3, 1],\n",
       " [2, 5, 6, 4, 3, 49, 4, 9, 3, 1],\n",
       " [2, 5, 6, 4, 3, 49, 6, 4, 3, 1],\n",
       " [2, 5, 6, 11, 3, 49, 6, 11, 3, 1],\n",
       " [2, 5, 6, 12, 3, 49, 6, 12, 3, 1],\n",
       " [2, 5, 4, 13, 3, 49, 6, 13, 3, 1],\n",
       " [2, 4, 6, 14, 3, 49, 6, 14, 3, 1],\n",
       " [2, 5, 6, 15, 3, 4, 6, 15, 3, 1],\n",
       " [2, 5, 4, 4, 3, 49, 6, 16, 3, 1]]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 7, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 9, 1, 1, 6, 1, 1, 1],\n",
       " [1, 1, 1, 10, 1, 1, 1, 10, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 6, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 5, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 49, 1, 1, 1, 1],\n",
       " [1, 1, 6, 36, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_labels_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n",
       " [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'bert_input': bert_inputs_final,\n",
    "    'bert_label': bert_labels_final,\n",
    "    'segment_label': segment_labels_final,\n",
    "    'relation': is_nexts_final\n",
    "})\n",
    "\n",
    "df_final.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
